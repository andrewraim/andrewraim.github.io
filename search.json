[
  {
    "objectID": "posts/2022-11-15-compoissonreg.html",
    "href": "posts/2022-11-15-compoissonreg.html",
    "title": "COMPoissonReg: Usage, the Normalizing Constant, and Other Computational Details",
    "section": "",
    "text": "I have been maintaining the COMPoissonReg package, which facilitates use of the Conway-Maxwell Poisson (CMP) distribution. The normalizing constant of the CMP distribution is central to many computations in the package. Users have occasionally reported strange results that can be traced back to problems computing the normalizing constant. Early versions computed the constant by truncating the series to an finite sum with a fixed number of terms; however, this may yield inaccurate results. We addressed this to some degree in more recent versions in an ad hoc way. In the latest version (0.8.0) of the package, these computations have been revisited in a more principled way. Kimberly Sellers and I recently completed a report describing the updated method, which also demonstrates general use of the package. A vignette version of this report will be included in the package and updated along with any changes in the software."
  },
  {
    "objectID": "posts/2020-08-03-jsm2020.html",
    "href": "posts/2020-08-03-jsm2020.html",
    "title": "Presentation for 2020 Joint Statistical Meetings",
    "section": "",
    "text": "For this year’s virtual JSM, I am presenting in the session listed here. Here are links to download the poster PDF and voiceover MP4. Note that the files are hosted on Google Drive: if this is not accessible to you, email me for copies of the files."
  },
  {
    "objectID": "posts/2025-11-05-vws-R.html",
    "href": "posts/2025-11-05-vws-R.html",
    "title": "vws: Vertical Weighted Strips in R using C++",
    "section": "",
    "text": "Raim, Livsey, and Irimata (2024+) consider a method of proposal construction for rejection sampling referred to as vertical weighted strips (VWS). Here, the target density is regarded as a weighted density; i.e., the product of a weight function and a simpler (“base”) density function. The practitioner majorizes the weight function by providing another function that bounds it from above. The majorizing function is combined with the base density to form a proposal distribution. In many cases, it is useful (and also more convenient) to partition the support of the target and use a piecewise majorizer. In this case, the proposal distribution can be regarded as a finite mixture whose components are correspond to the regions in the partition.\nThe vws package has been developed to support development of such proposals and their use in rejection sampling. Coding is primarily done in C++ using the provided API. Sampling functions may be then exposed in R via Rcpp for use in applications. A detailed guide to vws programming - including several worked examples - is provided in the package vignette.\nvws package\n\nDeployed on CRAN\nSource on Github\n\n\n\n\n\nReferences\n\nRaim, Andrew M., James A. Livsey, and Kyle M. Irimata. 2024+. “Rejection Sampling with Vertical Weighted Strips.” https://arxiv.org/abs/2401.09696."
  },
  {
    "objectID": "posts/2023-02-27-shiny-leaflet.html",
    "href": "posts/2023-02-27-shiny-leaflet.html",
    "title": "Interactive Map with Shiny and Leaflet",
    "section": "",
    "text": "There are powerful tools for working with geographical data in R. For example, the ggplot and sf packages can be used together to display such data. But what about interactive maps which support panning, zooming, and clicks to trigger actions (e.g., highlighting an area)?\nOne way to accomplish this is through Shiny and Leaflet. With this combination of tools, it is relatively easy to stand up an graphical interface. Shiny supports constructs beyond routine R programming which allow variables, displays, and computations to react to user inputs. Leaflet provides widgets for interactive maps.\nThere are already a number of interesting examples on the web, but here is something I put together.\n\nIt displays counties in Maryland along with some randomly placed markers, and displays the count of markers within each county as a choropleth. Users can remove existing markers or add new ones, which in turn triggers the counts to be recomputed and the choropleth to be updated."
  },
  {
    "objectID": "posts/2023-02-11-latex-tables-from-R.html",
    "href": "posts/2023-02-11-latex-tables-from-R.html",
    "title": "Generating Latex Tables in R",
    "section": "",
    "text": "Introduction\nPreparing Latex tables from results of a simulation study or other program can be tedious and time-consuming when done by hand. Especially when the process must be repeated many tables as results are updated to correct mistakes or to investigate unexpected findings. Hours spent manually adjusting formatting of table entries and typing ampersands between them could be spent in better ways. Fortunately, there are some excellent tools in R to help. Taking time to learn the tools and automate your table generation may be worth the investment. We will give a brief example in this post. This is only based on my experience; there are likely even better tools and methods that I have yet to learn.\n\n\nObjective\nOur objective will be to generate a Latex table for the first six rows of the airquality dataset.\n\nR&gt; head(airquality, n = 6)\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\nWe would also like to meet the following criteria.\n\nCriteria for formatting of the rendered Latex table.\n\nUse booktabs and make all separators horizontal lines.\nUse multiline columns to render column names.\nAdd several headers which group together columns in the display.\nSpecify a caption and a label which can be used in Latex references.\nRequest the table to be placed “here”.\n\nCriteria for table content.\n\nInclude row labels (observation number) in the table as a column.\nFormat numbers in a purposeful way.\nDisplay the Month and Day columns together as a date.\nCustomize column names to be more descriptive.\n\nEnsure that the generated Latex code is legible.\n\nPad strings in a given column so that they are nicely aligned in the code.\n\n\nTo do this, we will make use of the following tools.\n\nThe Tidyverse framework to manipulate tables.\nThe knitr package for reproducible research in R. In particular, we will use the kable function. to generate Latex from data frames.\nThe kableExtra package to handle some additional Latex work.\nThe built-in sprintf function to format table entries.\n\n\n\nPreparing the Data Frame\nFirst let us load the packages we will use.\n\nR&gt; library(knitr)\nR&gt; library(dplyr)\nR&gt; library(tibble)\nR&gt; library(stringr)\nR&gt; library(kableExtra)\n\nBefore converting to Latex, let us format the entries and column names as we would like them to appear.\n\ntbl = head(airquality, n = 6) %&gt;%\n    rownames_to_column(var = \"Num\") %&gt;%\n    mutate(Date = sprintf(\"%04d-%02d-%02d\", 1973, Month, Day)) %&gt;%\n    mutate(Solar.R = sprintf(\"%0.2e\", Solar.R)) %&gt;%\n    mutate(TempC = sprintf(\"%0.2f\", 5/9 * (Temp - 32))) %&gt;%\n    select(Num, Date, Ozone, Solar.R, Wind, TempC)\n\nWe have done the following.\n\nAssemble Month and Day into Date, where 1973 is the year which all observations were taken (according to the manual page for the dataset).\nConvert Solar.R to a string with the original value in scientific notation.\nConvert temperature Temp from Farenheit to Celcius and call the result TempC.\nUse tibble::rownames_to_column to include row labels in the table as the Obs column.\n\nThis produces the following table.\n\nR&gt; print(tbl)\n  Num       Date Ozone  Solar.R Wind TempC\n1   1 1973-05-01    41 1.90e+02  7.4 19.44\n2   2 1973-05-02    36 1.18e+02  8.0 22.22\n3   3 1973-05-03    12 1.49e+02 12.6 23.33\n4   4 1973-05-04    18 3.13e+02 11.5 16.67\n5   5 1973-05-05    NA       NA 14.3 13.33\n6   6 1973-05-06    28       NA 14.9 18.89\n\n\n\nGenerating Latex\nWe can now generate code to display out formatted table as Latex code.\n\nout = tbl %&gt;%\n    mutate(Solar.R = str_pad(Solar.R, width = 10, side = \"left\", pad = \"\\u00A0\")) %&gt;%\n    mutate(Wind = str_pad(Wind, width = 6, side = \"left\", pad = \"\\u00A0\")) %&gt;%\n    kable(format = \"latex\", booktabs = TRUE, linesep = \"\", align = c(\"rlrrrr\"),\n        caption = \"My formatted airquality table.\", label = \"airquality\",\n        col.names = c(\"Number\", \"Date\", \"Ozone (PPB)\", \"Radiation (Ly)\",\n            \"Wind (MPH)\", \"Temp (C)\")) %&gt;%\n    kable_styling(latex_options = c(\"hold_position\")) %&gt;%\n    add_header_above(c(\"Observation\" = 2, \"Solar\" = 2, \"Weather\" = 2))\n\nWe have done the following.\n\nLeft-pad the Solar.R and Wind fields with the unicode “nbsp” character, which will render as a space in our resulting code rather than be ignored, as a regular space would.\nUse kable to generate a Latex table from our data frame. We have specified options such as cell alignments, the caption, and the label, which should be familiar to Latex users. We have also specified descriptive column names here.\nThe option hold_position specifies the option !h for how Latex should place the table.\nThe function add_header_above specifies a layer of headers above the column names. These respectively have text “Observation”, “Solar” and “Weather”, and are each two columns wide.\n\nPrinting the result yields the following Latex code.\n\nR&gt; print(out)\n\\begin{table}[!h]\n\\centering\n\\caption{\\label{tab:airquality}My formatted airquality table.}\n\\centering\n\\begin{tabular}[t]{rlrrrr}\n\\toprule\n\\multicolumn{2}{c}{Observation} & \\multicolumn{2}{c}{Solar} & \\multicolumn{2}{c}{Weather} \\\\\n\\cmidrule(l{3pt}r{3pt}){1-2} \\cmidrule(l{3pt}r{3pt}){3-4} \\cmidrule(l{3pt}r{3pt}){5-6}\nNumber & Date & Ozone (PPB) & Radiation (Ly) & Wind (MPH) & Temp (C)\\\\\n\\midrule\n1 & 1973-05-01 & 41 &   1.90e+02 &    7.4 & 19.44\\\\\n2 & 1973-05-02 & 36 &   1.18e+02 &      8 & 22.22\\\\\n3 & 1973-05-03 & 12 &   1.49e+02 &   12.6 & 23.33\\\\\n4 & 1973-05-04 & 18 &   3.13e+02 &   11.5 & 16.67\\\\\n5 & 1973-05-05 & NA &         NA &   14.3 & 13.33\\\\\n6 & 1973-05-06 & 28 &         NA &   14.9 & 18.89\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\nWe now have a Latex table environment - which is fairly human-readable - that can be copy/pasted into a Latex document, or even included programmatically using Sweave."
  },
  {
    "objectID": "posts/2025-09-23-saevws.html",
    "href": "posts/2025-09-23-saevws.html",
    "title": "Self-Tuned VWS within Gibbs and Small Area Estimation",
    "section": "",
    "text": "Raim, Irimata, and Livsey (2025+) consider an application of the vertical weighted strips (VWS) method (Raim, Livsey, and Irimata 2024+) to small area estimation (SAE). SAE is used in official statistics to augment estimates from sample surveys (“direct estimates”) with a model. “Small areas” are cross-sections of a population - based on geography or other characteristics - where the sample size from the survey is small.\nIn particular, You (2021) presents an SAE model with regressions on both direct point estimates and corresponding variance estimates. A Gibbs sampler is proposed for Bayesian analysis, with one family of conditionals being an unfamiliar distribution that arises from assuming a lognormal regression in the variance model. VWS may be used to generate from this family for each small area. We find that some of the latent variances mix poorly using the independent Metropolis-Hastings step considered in You (2021), which is a routine choice for use within Gibbs samplers. In these cases, mixing is seen to be greatly improved by taking exact draws from the conditionals with rejection sampling.\nIt is computationally burdensome to construct new VWS proposals for each small area over the course of many Gibbs sampling iterations. To address this, we consider a rule-of-thumb to adjust the proposals as the chain evolves. In particular, knots are added using rejected draws when the probability of rejection is too large. Knots are removed when the probability of rejection is sufficiently small and their corresponding regions are found to have a low contribution. The number of adjustments is seen to diminish as the chain moves to the target posterior distribution.\n\n\n\n\nReferences\n\nRaim, Andrew M., Kyle M. Irimata, and James A. Livsey. 2025+. “Self-Tuned Rejection Sampling Within Gibbs and a Case Study in Small Area Estimation.” https://arxiv.org/abs/2509.17155.\n\n\nRaim, Andrew M., James A. Livsey, and Kyle M. Irimata. 2024+. “Rejection Sampling with Vertical Weighted Strips.” https://arxiv.org/abs/2401.09696.\n\n\nYou, Yong. 2021. “Small Area Estimation Using Fay-Herriot Area Level Model with Sampling Variance Smoothing and Modeling.” Survey Methodology 47 (2). http://www.statcan.gc.ca/pub/12-001-x/2021002/article/00007-eng.htm."
  },
  {
    "objectID": "posts/2024-01-19-vws.html",
    "href": "posts/2024-01-19-vws.html",
    "title": "Rejection Sampling with Vertical Weighted Strips",
    "section": "",
    "text": "Rejection sampling is a classical algorithm (Neumann 1951) to generate variates from a target distribution where a constructive sampling method—e.g., by composing other random variables—may not be apparent. A benefit of rejection sampling is that accepted draws follow the target distribution exactly. However, coming up with a good proposal for rejection sampling can take some creativity. A poor choice can result in extremely low acceptance rates where practically no draws are accepted from a very large number of candidates.\nRaim, Livsey, and Irimata (2024+) explore an approach to construct proposals by regarding the target as a weighted density and majorizing the weight function. This yields another weighted density whose unnormalized form can be used as an envelope in the acceptance ratio. If designed consciously, the normalized form can also be used to draw candidate variates to complete the rejection sampler. An improved proposal can be obtained by partitioning the support and majorizing within each region. Here the proposal is a finite mixture. The method is referred to as “vertical weighted strips” because it can be regarded as an extension of the vertical strips method (Devroye 1986, chap. VIII; Martino, Luengo, and Míguez 2018, sec. 3.6), with the weighted form introducing an additional degree of flexibility for algorithm development.\n\n\n\n\nReferences\n\nDevroye, Luc. 1986. Non-Uniform Random Variate Generation. Springer.\n\n\nMartino, Luca, David Luengo, and Joaquín Míguez. 2018. Independent Random Sampling Methods. Springer. https://doi.org/https://dx.doi.org/10.1007/978-3-319-72634-2.\n\n\nNeumann, John von. 1951. “Various Techniques in Connection with Random Digits.” In Monte Carlo Methods, edited by A. S. Householder, G. E. Forsythe, and H. H. Germond, 36–38. National Bureau of Standards Applied Mathematics Series. U.S. Government Printing Office, Washington, DC.\n\n\nRaim, Andrew M., James A. Livsey, and Kyle M. Irimata. 2024+. “Rejection Sampling with Vertical Weighted Strips.” https://arxiv.org/abs/2401.09696."
  },
  {
    "objectID": "posts/2023-05-01-sendmail-R.html",
    "href": "posts/2023-05-01-sendmail-R.html",
    "title": "Sending Mail from R",
    "section": "",
    "text": "Here is a quick note on sending mail from R. For example, this could be useful in a long running job to alert yourself when it completes or halts because of an error. The sendmailR package provides a general interface to mail.\nHere is an example where we use sendmailR to send mail to the local mailbox of our user (araim) on the host machine. This assumes we have a sendmail server set up to relay local mail and that araim has a mailbox.\nThe following call in R sends a message.\n\nresult = sendmail(\n    from = \"sendmailR\",\n    to = \"araim\",\n    subject = \"Hello from R\",\n    msg = \"This is an email from R using the sendmailR package.\",\n    control = list(smtpServer = \"localhost\")\n)\n\nHere is how the message appears when viewed with mutt.\ni:Exit  -:PrevPg  &lt;Space&gt;:NextPg  v:View Attachm.  d:Del  r:Reply  j:Next  ?:Help\nDate: Tue, 02 May 2023 09:24:31 -0000\nFrom: sendmailR@localhost\nTo: araim@localhost\nSubject: Hello from R\n\n[-- Attachment #1 --]\n[-- Type: text/plain; charset=us-ascii, Encoding: 7bit, Size: 0.1K --]\n\nThis is an email from R using the sendmailR package.\n\n-N +- 1155/1155: sendmailR@localhost   Hello from R                      -- (all)\nHere is an example of sending mail on an exception using withCallingHandlers. Here we use sys.calls to give some context about the call that led to the error.\n\nf = function(x) { stop(\"An exception!\") }\ng = function(x) { f(x) }\n\nwithCallingHandlers({\n    g(1:10)\n}, error = function(e) {\n    result = sendmail(\n        from = \"sendmailR\",\n        to = \"araim\",\n        subject = \"Exception from R\",\n        msg = sprintf(\"%d: %s\", seq_along(sys.calls()), sys.calls()),\n        control = list(smtpServer = \"localhost\")\n    )\n})\n\nAnd here is how the message appears when viewed with mutt.\ni:Exit  -:PrevPg  &lt;Space&gt;:NextPg  v:View Attachm.  d:Del  r:Reply  j:Next  ?:Help\nDate: Tue, 02 May 2023 10:06:56 -0000\nFrom: sendmailR@localhost\nTo: araim@localhost\nSubject: Exception from R\n\n[-- Attachment #1 --]\n[-- Type: text/plain; charset=us-ascii, Encoding: 7bit, Size: 0.6K --]\n\n1: withCallingHandlers({\n   g(1:10)\n}, error = function(e) {\n   result = sendmail(from = \"sendmailR\", to = \"araim\", subject = \"Exception from\n+R\", msg = sprintf(\"%d: %s\", seq_along(sys.calls()), sys.calls()), control =\n+list(smtpServer = \"localhost\"))\n})\n2: g(1:10)\n3: f(x)\n4: stop(\"An exception!\")\n5: .handleSimpleError(function (e) {\n   result = sendmail(from = \"sendmailR\", to = \"araim\", subject = \"Exception from\n+R\", msg = sprintf(\"%d: %s\", seq_along(sys.calls()), sys.calls()), control =\n+list(smtpServer = \"localhost\"))\n}, \"An exception!\", base::quote(f(x)))\n6: h(simpleError(msg, call))\n\n-N +- 1162/1162: sendmailR@localhost   Exception from R                  -- (all)"
  },
  {
    "objectID": "posts/2024-12-11-stats4.html",
    "href": "posts/2024-12-11-stats4.html",
    "title": "Numerical Maximum Likelihood with the stats4 Package",
    "section": "",
    "text": "1 Introduction\nThe stats4 package included in R provides a layer of usability on top of optim for numerial maximum likelihood (ML). For example, the user can request an estimate of the covariance matrix associated with the ML estimates rather than computing it manually. According to Henningsen and Toomet (2011), stats4 has been bundled into R since 2003, but I only found out about it recently. In this post, we will present some brief examples using stats4. Note that the maxLik package presented by Henningsen and Toomet (2011) covers similar ground with additional functionality; therefore it may also be of interest.\n\n\n2 Poisson Regression Example\nConsider a Poisson regression setting with \\(Y_i \\sim \\text{Poisson}(\\lambda_i)\\), independently distributed for \\(i = 1, \\ldots, n\\) where \\(\\lambda_i = \\exp(x_i^\\top \\beta)\\) for covariate \\(x_i \\in \\mathbb{R}^d\\) and unknown coefficient \\(\\beta \\in \\mathbb{R}^d\\).\nLet us simulate a dataset from this setting.\n\nset.seed(1235)\nn = 200\nx = rnorm(n)\nX = cbind(1, x)\nd = ncol(X)\nbeta_true = c(1, -0.25)\nlambda_true = exp(X %*% beta_true)\ny = rpois(n, lambda_true)\n\nOf course we can use the glm function to obtain the MLE \\(\\hat{\\beta}\\) in this setting. Let us do that as a point of comparison.\n\nglm_out = glm(y ~ x, family = poisson)\nsummary(glm_out)\n\n\nCall:\nglm(formula = y ~ x, family = poisson)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.98109    0.04392  22.338  &lt; 2e-16 ***\nx           -0.25893    0.03908  -6.626 3.44e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 272.04  on 199  degrees of freedom\nResidual deviance: 228.85  on 198  degrees of freedom\nAIC: 749.28\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n3 Using the mle Function in stats4\nRecall that the loglikelihood for this model is  \\[\n\\log \\mathcal{L}(\\beta) = \\sum_{i=1}^n \\Big\\{ y_i \\log \\lambda_i - \\lambda_i - \\log \\Gamma(y_i + 1) \\Big\\}.\n\\]  If this were a nonstandard likelihood, we would need to be able to code and optimize it ourselves. The stats4 package allows us to do this. Here are the arguments of the stats4::mle function which may be used to fit the MLE.\n\nstr(mle, give.attr = F)\n\nfunction (minuslogl, start, optim = stats::optim, method = if (!useLim) \"BFGS\" else \"L-BFGS-B\", \n    fixed = list(), nobs, lower, upper, ...)  \n\n\nNote that the first argument expects the negative loglikelihood; the optimization is then carried out as a minimization. Let us code it.\n\nnloglik = function(beta = numeric(d)) {\n    -sum(dpois(y, lambda = exp(X %*% beta), log = TRUE))\n}\n\nmle_out = mle(nloglik, method = \"L-BFGS-B\", nobs = length(y))\n\nA variety of included accessors can be used to work with the result.\n\nsummary(mle_out)\n\nMaximum likelihood estimation\n\nCall:\nmle(minuslogl = nloglik, method = \"L-BFGS-B\", nobs = length(y))\n\nCoefficients:\n        Estimate Std. Error\nbeta1  0.9810935 0.04392105\nbeta2 -0.2589335 0.03907591\n\n-2 log L: 745.2773 \n\ncoef(mle_out)\n\n     beta1      beta2 \n 0.9810935 -0.2589335 \n\nlogLik(mle_out)\n\n'log Lik.' -372.6387 (df=2)\n\nAIC(mle_out)\n\n[1] 749.2773\n\nBIC(mle_out)\n\n[1] 755.874\n\nconfint(mle_out, level = 0.90)\n\nProfiling...\n\n\n             5 %      95 %\nbeta1  0.9079239  1.052437\nbeta2 -0.3230551 -0.194514\n\nvcov(mle_out)\n\n             beta1        beta2\nbeta1 0.0019290582 0.0004114616\nbeta2 0.0004114616 0.0015269270\n\nprofile(mle_out)\n\nAn object of class \"profile.mle\"\nSlot \"profile\":\n$beta1\n            z par.vals.beta1 par.vals.beta2\n1  -3.0192356      0.8453337     -0.2894697\n2  -2.5258203      0.8679603     -0.2841583\n3  -2.0285449      0.8905870     -0.2789366\n4  -1.5273678      0.9132136     -0.2738039\n5  -1.0222468      0.9358402     -0.2687598\n6  -0.5131396      0.9584668     -0.2638030\n7   0.0000000      0.9810935     -0.2589335\n8   0.5172066      1.0037201     -0.2541504\n9   1.0385329      1.0263467     -0.2494528\n10  1.5640206      1.0489733     -0.2448400\n11  2.0937147      1.0716000     -0.2403112\n12  2.6276606      1.0942266     -0.2358654\n\n$beta2\n            z par.vals.beta1 par.vals.beta2\n1  -2.5855783      0.9478931     -0.3595864\n2  -2.0668516      0.9555116     -0.3394558\n3  -1.5489482      0.9626386     -0.3193253\n4  -1.0318503      0.9692764     -0.2991947\n5  -0.5155395      0.9754273     -0.2790641\n6   0.0000000      0.9810935     -0.2589335\n7   0.5147983      0.9862771     -0.2388029\n8   1.0288654      0.9909803     -0.2186724\n9   1.5422257      0.9952050     -0.1985418\n10  2.0549007      0.9989531     -0.1784112\n11  2.5669123      1.0022264     -0.1582806\n12  3.0782832      1.0050266     -0.1381501\n\n\nSlot \"summary\":\nMaximum likelihood estimation\n\nCall:\nmle(minuslogl = nloglik, method = \"L-BFGS-B\", nobs = length(y))\n\nCoefficients:\n        Estimate Std. Error\nbeta1  0.9810935 0.04392105\nbeta2 -0.2589335 0.03907591\n\n-2 log L: 745.2773 \n\n\n\n\n4 Fixed Parameters\nAn argument of mle that can be useful is fixed, where we can specify some parameters to be held fixed during the optimization. In our example with \\(\\beta = (\\beta_1, \\beta_2)\\) let us consider another fit with the slope coefficient fixed at \\(\\beta_2 = 0\\) .\n\nmle0_out = mle(nloglik, method = \"L-BFGS-B\", nobs = length(y),\n    fixed = list(beta = c(NA, 0)))\nprint(mle0_out)\n\n\nCall:\nmle(minuslogl = nloglik, method = \"L-BFGS-B\", fixed = list(beta = c(NA, \n    0)), nobs = length(y))\n\nCoefficients:\n   beta1    beta2 \n1.011601 0.000000 \n\n\nNote that NA was associated with \\(\\beta_1\\) to indicate that it should not remain fixed. We can compute a likelihood ratio test of the hypothesis \\(H_0: \\beta_2 = 0\\) versus \\(H_1: \\beta_2 \\neq 0\\) using the two fits mle_out and mle0_out.\n\nlrt = 2 * (logLik(mle_out) - logLik(mle0_out))\npval = pchisq(lrt, df = 1, lower.tail = F)\ncat(\"LRT was computed with test statistic\", lrt, \"and p-value\", pval, \".\\n\")\n\nLRT was computed with test statistic 43.19441 and p-value 4.956214e-11 .\n\n\n\n\n5 Transforming Parameters\nA convenient property of ML estimates (MLEs) is the invariance property: suppose the MLE of \\(\\beta\\) is \\(\\hat{\\beta}\\), then the MLE of a function \\(g(\\beta)\\) is \\(g(\\hat{\\beta})\\). An associated variance estimate is given by  \\[\n\\widehat{\\text{Var}}(g(\\hat{\\beta})) =\n[J(\\hat{\\beta})]\n\\widehat{\\text{Var}}(\\hat{\\beta})\n[J(\\hat{\\beta})]^\\top\n\\]  where \\(J(\\beta) = \\partial g(\\beta) / \\partial \\beta\\) is the Jacobian of the transformation. Here is an R function to compute the transformed variance. Note that we use the numDeriv package.\n\nvcov_tx = function(object, tx) {\n    J = numDeriv::jacobian(func = tx, x = coef(object))\n    V = vcov(object)\n    J %*% tcrossprod(V, J)\n}\n\nAs an example, let us estimate each \\(\\lambda_i = \\exp(x_i^\\top \\beta)\\) and its variance, and construct an associated confidence interval with level 90% nominal coverage level.\n\nalpha = 0.10\ntx = function(beta) { exp(X %*% beta) }\ntx_hat = tx(coef(mle_out))\nsd_tx_hat = sqrt(diag(vcov_tx(mle_out, tx)))\ntx_lo = tx_hat - qnorm(1 - alpha/2) * sd_tx_hat\ntx_hi = tx_hat + qnorm(1 - alpha/2) * sd_tx_hat\n\nHere are the results in a data frame.\n\nlibrary(tidyverse)\nlibrary(kableExtra)\ndata.frame(EST = tx_hat, SD = sd_tx_hat, LO = tx_lo, HI = tx_hi) %&gt;%\n    mutate(TRUTH = lambda_true) %&gt;%\n    head() %&gt;%\n    kbl(booktabs = T)\n\n\n\n\nEST\nSD\nLO\nHI\nTRUTH\n\n\n\n\n3.195763\n0.1463983\n2.954960\n3.436567\n3.236515\n\n\n3.720229\n0.2166837\n3.363816\n4.076642\n3.747967\n\n\n2.064235\n0.1344154\n1.843142\n2.285329\n2.122321\n\n\n2.591277\n0.1170420\n2.398760\n2.783794\n2.643374\n\n\n2.589646\n0.1170497\n2.397117\n2.782176\n2.641767\n\n\n1.718365\n0.1510794\n1.469862\n1.966869\n1.777932\n\n\n\n\n\n\n\n\n\n\n\n\n\n6 References\n\nHenningsen, Arne, and Ott Toomet. 2011. “maxLik: A Package for Maximum Likelihood Estimation in R.” Computational Statistics 26 (3): 443–58. https://doi.org/10.1007/s00180-010-0217-1."
  },
  {
    "objectID": "bib/reports.html",
    "href": "bib/reports.html",
    "title": "Andrew M. Raim",
    "section": "",
    "text": "@techreport{fntl2024,\n  title = \"{fntl}: Numerical Tools for {Rcpp} and Lambda Functions\",\n  author = \"Andrew~M. Raim\",\n  type = \"Research Report Series: Computing\",\n  number = {\\#2024-01},\n  institution = \"Center for Statistical Research and Methodology,\n                 U.S.~Census Bureau\",\n  year = 2024,\n  url = \"https://www.census.gov/library/working-papers/2024/adrm/RRC2024-01.html\",\n}\n\n@techreport{BilingualTrainingAnalysis2024,\n  title = \"A Multinomial Analysis of Bilingual Training and Nonresponse\n           Followup Contact Rates in the 2020 Decennial Census\",\n  author = \"Andrew~M. Raim and Renee Ellis and Mikelyn Meyers\",\n  type = \"Study Series: Statistics\",\n  number = {\\#2024-01},\n  institution = \"Center for Statistical Research and Methodology,\n                 U.S.~Census Bureau\",\n  year = 2024,\n  url = \"https://www.census.gov/library/working-papers/2024/adrm/SSS2024-01.html\",\n}\n\n@techreport{BivariateOrdinalMaps2023,\n  title = \"A Comparison of Map Usability via Bivariate Ordinal Analysis\",\n  author = \"Andrew~M. Raim and Elizabeth Nichols\",\n  type = \"Study Series: Statistics\",\n  number = {\\#2023-01},\n  institution = \"Center for Statistical Research and Methodology,\n                 U.S.~Census Bureau\",\n  year = 2023,\n  url = \"https://www.census.gov/library/working-papers/2023/adrm/SSS2023-01.html\",\n}\n\n@techreport{COMPoissonReg2022,\n  title = \"{COMPoissonReg}: Usage, the Normalizing Constant, and Other\n           Computational Details\",\n  author = \"Andrew~M. Raim and Kimberly~F. Sellers\",\n  type = \"Research Report Series: Computing\",\n  number = {\\#2022-01},\n  institution = \"Center for Statistical Research and Methodology,\n                 U.S.~Census Bureau\",\n  year = 2022,\n  url = \"https://www.census.gov/library/working-papers/2022/adrm/RRC2022-01.html\",\n}\n\n@techreport{DPSimulation2022,\n  title = \"Evaluation of {B}ayesian Hierarchical Models of Differentially\n           Private Data Based on an Approximate Data Model\",\n  author = \"Kyle~M. Irimata and Andrew~M. Raim and Ryan Janicki and James~A.\n            Livsey and Scott~H. Holan\",\n  type = \"Research Report Series: Statistics\",\n  number = {\\#2022-05},\n  institution = \"Center for Statistical Research and Methodology,\n                 U.S.~Census Bureau\",\n  year = 2022,\n  url = \"https://www.census.gov/library/working-papers/2022/adrm/RRS2022-05.html\",\n}\n\n@techreport{SFReader2022,\n  title = \"Browsing the 2010 {C}ensus {SF2} Summary File with {R}\",\n  author = \"Andrew~M. Raim and James A. Livsey and Kyle M. Irimata\",\n  type = \"Study Series: Computing\",\n  number = {\\#2022-01},\n  institution = \"Center for Statistical Research and Methodology,\n                 U.S.~Census Bureau\",\n  year = 2022,\n  url = \"https://www.census.gov/library/working-papers/2022/adrm/SSC2022-01.html\"\n}\n\n@techreport{DirectSamplingDAS2021,\n  title = \"Direct Sampling in {B}ayesian Regression Models with Additive\n           Disclosure Avoidance Noise\",\n  author = \"Andrew~M. Raim\",\n  type = \"Research Report Series: Statistics\",\n  number = {\\#2021-01},\n  institution = \"Center for Statistical Research and Methodology,\n                 U.S.~Census Bureau\",\n  year = 2021,\n  url = \"https://www.census.gov/library/working-papers/2021/adrm/RRS2021-01.html\",\n}\n\n@incollection{NicholsEtAl2020,\n  booktitle = \"Human Aspects of {IT} for the Aged Population. Technologies,\n               Design and User Experience\",\n  title = \"Attitudinal and Behavioral Differences Between Older and Younger\n           Adults Using Mobile Devices\",\n  author = \"Elizabeth Nichols and Erica Olmsted-Hawala and Andrew Raim and\n            Lin Wang\",\n  year = 2020,\n  publisher = \"Springer Nature Switzerland AG\",\n  doi = \"https://doi.org/10.1007/978-3-030-50252-2_25\"\n}\n\n@techreport{MailDelivery2019,\n  title = \"2020 Census Research and Testing Report: The Effect of the Mail\n           Delivery Date on Survey Login Rates and Helpline Call Rates\",\n  author = \"Elizabeth Nichols and Sarah Konya amd Rachel Horwitz and\n            Andrew Raim\",\n  type = \"U.S. Census Bureau, Research and Methodology Directorate,\n          Center for Behavioral Science Methods Research Report Series\n          (Survey Methodology)\",\n  number = {\\#2019-01},\n  institution = \"U.S.~Census Bureau\",\n  year = 2019,\n  url = {https://www.census.gov/content/dam/Census/library/working-papers/2019/adrm/rsm2019-01.pdf},\n}\n\n@misc{RaimThesis2014,\n  author = \"Andrew~M. Raim\",\n  title = \"Computational Methods in Finite Mixtures using\n           Approximate Information and Regression Linked to the Mixture Mean\",\n  howpublished = \"Ph.D. Thesis, Department of Mathematics and Statistics,\n                  University of Maryland, Baltimore County\",\n  year = 2014,\n  preprint = {https://andrewraim.github.io/downloads/doc/araim-thesis-20140425.pdf},\n  url = \"http://search.proquest.com/docview/1552496226\",\n}\n\n@techreport{MAFErrorModel2015,\n  title = \"Selection of Predictors to Model Coverage Errors in the\n           {M}aster {A}ddress {F}ile\",\n  author = \"Andrew~M. Raim and Marissa~N. Gargano\",\n  type = \"Research Report Series: Statistics\",\n  number = {\\#2015-04},\n  institution = \"Center for Statistical Research and Methodology,\n                 U.S.~Census Bureau\",\n  year = 2015,\n  url = \"https://www.census.gov/library/working-papers/2015/adrm/rrs2015-04.html\",\n}\n\n@techreport{OverdispersionModelsInR2015,\n  title = \"Modeling Overdispersion in {R}\",\n  author = \"Andrew~M. Raim and Nagaraj~K. Neerchal and Jorge~G. Morel\",\n  number = {HPCF-2015-1},\n  institution = \"UMBC High Performance Computing Facility,\n                 University of Maryland, Baltimore County\",\n  year = 2015,\n  webnotes = {For code and examples, see\n    \\href{http://github.com/andrewraim/OverdispersionModelsInR}{OverdispersionModelsInR} on Github.\n  },\n  url = \"https://hpcf.umbc.edu/publications\",\n  pdf = \"http://hpcf-files.umbc.edu/research/papers/OverdispersionModelsInR.pdf\",\n}\n\n@techreport{pbdRtara2013,\n  author = \"Andrew~M. Raim\",\n  title = \"Introduction to Distributed Computing with {pbdR} at the {UMBC}\n           {H}igh {P}erformance {C}omputing {F}acility\",\n  number = {HPCF-2013-2},\n  institution = \"UMBC High Performance Computing Facility,\n                 University of Maryland, Baltimore County\",\n  year = 2013,\n  webnotes = {Files for download with this report:\n    \\href{http://userpages.umbc.edu/~gobbert/papers/pbdRtara2013.tar.gz}{pbdRtara2013.tar.gz}\n  },\n  url = \"https://hpcf.umbc.edu/publications\",\n  pdf = \"http://hpcf-files.umbc.edu/research/papers/pbdRtara2013.pdf\",\n}\n\n@techreport{RaimLiuNeerchalMorel2012,\n  author = \"Andrew~M. Raim and Minglei Liu and Nagaraj~K. Neerchal\n            and Jorge~G. Morel\",\n  title = \"An {A}pproximate {F}isher {S}coring {A}lgorithm for\n           Finite Mixtures of Multinomials\",\n  year = 2012,\n  institution = \"UMBC High Performance Computing Facility,\n                 University of Maryland, Baltimore County\",\n  number = \"HPCF-2012-14\",\n  url = \"https://hpcf.umbc.edu/publications\",\n  pdf = \"http://hpcf-files.umbc.edu/research/papers/RaimLiuNeerchalMorel2012.pdf\"\n}\n\n@techreport{RaimGobbert2010Poisson,\n  author = \"Andrew~M. Raim and Matthias~K. Gobbert\",\n  title = \"Parallel Performance Studies for an Elliptic Test Problem \n           on the Cluster tara\",\n  number = \"HPCF-2010-2\",\n  institution = \"UMBC High Performance Computing Facility,\n                 University of Maryland, Baltimore County\",\n  year = 2010,\n  url = \"https://hpcf.umbc.edu/publications\",\n  pdf = \"http://hpcf-files.umbc.edu/research/papers/RaimGobbert2010Poisson.pdf\",\n}\n\n\n@inproceedings{MorrisRaim2023,\n  title = \"Comparing trial and variable association in contingency table data\n           using multinomial models for clustered data\",\n  author = \"Darcy Steeg Morris and Andrew~ M. Raim\",\n  booktitle = \"37th International Workshop on Statistical Modelling\",\n  year = 2023,\n  pages = \"536--542\",\n  editor = \"Elisabeth Bergherr and Andreas Groll and Andreas Mayr\",\n  url = \"https://iwsm2023.statistik.tu-dortmund.de\"\n}\n\n@inproceedings{JSM2018-CMM,\n  title = \"Introducing a {C}onway-{M}axwell-Multinomial Distribution for \n           Flexible Modeling of Categorical Data\",\n  author = \"Darcy Steeg Morris and Andrew~M. Raim and Kimberly~F. Sellers\",\n  booktitle = \"JSM Proceedings, Biometrics Section\",\n  year = 2018,\n  pages = \"716--733\",\n  publisher = \"American Statistical Association\",\n  address = \"Alexandria, VA\",\n  preprint = {https://andrewraim.github.io/downloads/doc/MorrisRaimSellersJSM2018.pdf},\n}\n\n@inproceedings{JSM2017-STCOS,\n  title = \"A Model Selection Study for Spatio-Temporal Change of Support\",\n  author = \"Andrew~M. Raim and Scott~H. Holan and Jonathan~R. Bradley and\n            Christopher~K. Wikle\",\n  booktitle = \"JSM Proceedings, Government Statistics Section\",\n  year = 2017,\n  pages = \"1524--1540\",\n  publisher = \"American Statistical Association\",\n  address = \"Alexandria, VA\",\n  preprint = {https://andrewraim.github.io/downloads/doc/JSM2017-STCOS.pdf},\n}\n\n@inproceedings{RaimJSM2016,\n  title = \"Informing Maintenance to the {U.S.}~{C}ensus {B}ureau's {M}aster\n           {A}ddress {F}ile with Statistical Decision Theory\",\n  author = \"Andrew~M. Raim\",\n  booktitle = \"JSM Proceedings, Government Statistics Section\",\n  year = 2016,\n  pages = \"648--659\",\n  publisher = \"American Statistical Association\",\n  address = \"Alexandria, VA\",\n  preprint = {https://andrewraim.github.io/downloads/doc/RaimJSM2016.pdf},\n}\n\n@inproceedings{HeimRaimJSM2016,\n  title = \"Predicting Coverage Error on the {M}aster {A}ddress {F}ile using\n           Spatial Modeling Methods at the Block Level\",\n  author = \"Krista Heim and Andrew~M. Raim\",\n  booktitle = \"JSM Proceedings, Survey Research Methods Section\",\n  year = 2016,\n  pages = \"1541--1555\",\n  publisher = \"American Statistical Association\",\n  address = \"Alexandria, VA\",\n  preprint = {https://andrewraim.github.io/downloads/doc/HeimRaimJSM2016.pdf},\n}\n\n@inproceedings{MixLinkJSM2015,\n  title = \"Bayesian Analysis of Overdispersed Binomial Data using Mixture\n           Link Regression\",\n  author = \"Andrew~M. Raim and Marissa~N. Gargano and Nagaraj~K. Neerchal\n            and Jorge~G. Morel\",\n  booktitle = \"JSM Proceedings, Statistical Computing Section\",\n  year = 2015,\n  pages = \"2794--2808\",\n  publisher = \"American Statistical Association\",\n  address = \"Alexandria, VA\",\n  preprint = {https://andrewraim.github.io/downloads/doc/MixLinkJSM2015.pdf},\n}\n\n@inproceedings{ApproxFIMJSM2014,\n  title = \"Large Cluster Approximation to the Finite Mixture Information\n           Matrix with an Application to Meta-Analysis\",\n  author = \"Andrew~M. Raim and Nagaraj~K. Neerchal and Jorge~G. Morel\",\n  booktitle = \"JSM Proceedings, Statistical Computing Section\",\n  year = 2014,\n  pages = \"4025--4037\",\n  publisher = \"American Statistical Association\",\n  address = \"Alexandria, VA\",\n  preprint = {https://andrewraim.github.io/downloads/doc/ApproxFIMJSM2014.pdf},\n}\n\n@inproceedings{RaimNeerchalJSM2013,\n  title = \"Modeling Overdispersion in Binomial Data with Regression Linked\n           to a Finite Mixture Probability of Success\",\n  author = \"Andrew~M. Raim and Nagaraj~K. Neerchal\",\n  booktitle = \"JSM Proceedings, Statistical Computing Section\",\n  year = 2013,\n  pages = \"2760--2774\",\n  publisher = \"American Statistical Association\",\n  address = \"Alexandria, VA\",\n  preprint = {https://andrewraim.github.io/downloads/doc/RaimNeerchalJSM2013.pdf},\n}\n\n@inproceedings{RaimFlemingNeerchalJSM2012,\n  title = \"An Analysis of Categorical Injury Data using Mixtures of\n           Multinomials\",\n  author = \"Andrew~M. Raim and Brandon~E. Fleming and Nagaraj~K. Neerchal\",\n  booktitle = \"JSM Proceedings, Statistical Computing Section\",\n  year = 2012,\n  pages = \"2444--2458\",\n  publisher = \"American Statistical Association\",\n  address = \"Alexandria, VA\",\n  preprint = {https://andrewraim.github.io/downloads/doc/RaimFlemingNeerchalJSM2012.pdf},\n}"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "R packages",
    "section": "",
    "text": "R packages\n\nallocation: Exact optimal allocation algorithms for stratified sampling. [cran | repo]\nCOMMultReg: Conway-Maxwell multinomial regression. [repo]\nCOMPoissonReg: Conway-Maxwell Poisson regression. [cran | repo]\nDirectSampling: Direct sampling with a step function. Also see DirectSampling.jl for a Julia implementation. [repo]\nfntl: Numerical Tools for Rcpp and Lambda Functions. [cran | repo]\nldr: Methods for likelihood-based dimension reduction in regression. [cran | repo]\nManifoldOptim: An R interface to the ROPTLIB library for Riemannian manifold optimization. [cran | repo]\nmixlink: Mixture link regression. [cran | repo]\nOverdispersionModelsInR: Some tools to help with MLE computation, especially for non-standard likelihoods found in overdispersion models. [repo]\nraim: a collection of my utility functions. [repo]\nsfreader: Read summary files with census data. Currently focused on the 2010 SF2 summary file. [repo]\nstcos: Space-time change of support. [cran | repo]\nvws: Vertical Weighted Strips in R using C++. [cran | repo]\n\n\n\nTemplates for Documents\n\nbeamerthemeraim: A custom Beamer theme. [repo]\nformula-sheet: Formula sheets in Latex. [www]\nlatex-business-card: Business cards in Latex. [www]\nlatex-resume: CV / resume in Latex. [www]\nquarto-templates: Documents such as an article, slides, and a vignette in Quarto. [repo]\nsweave-templates: Slides and articles using Sweave via knitr. [repo]\n\n\n\nUtilities\n\nmd-util: some command line scripts and templates to facilitate compiling markdown to various formats (pdf, html, beamer) using Pandoc. [repo]\nworker: a tool to help run repetitive computational studies such as statistical simulations. [repo]"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Andrew M. Raim",
    "section": "",
    "text": "Selected slides and posters. In cases where similar material was presented in several places, only one version of the files are given here.\n\nVertical Weighted Strips in R. [poster]\nRejection sampling for weighted densities by majorization. [slides] [poster]\nDirect sampling in Bayesian regression models with additive disclosure avoidance noise. [slides]\nSample size selection in continuation-ratio logit models. [poster]\nA statistical comparison of call volume uniformity due to mailing strategy. [slides]\nAn R package for spatio-temporal change of support. [slides]\nA flexible zero-inflated model to address data dispersion. [slides].\nInforming maintenance to the U.S. Census Bureau’s Master Address File with statistical decision theory. [poster].\nAn extension of generalized linear models to finite mixture outcomes. [slides].\nSelection of predictors to model coverage errors in the Master Address File. [slides].\nLarge cluster approximation to the information matrix using complete data. [slides].\nZero-inflated regression modeling for coverage errors of the Master Address File. [slides].\nAn analysis of categorical injury data using mixtures of multinomials. [slides].\nAn approximate Fisher scoring algorithm for finite mixtures of multinomials. [slides] [poster].\nMaximum likelihood estimation of multinomial mixture models using high performance computing. [slides]"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Manuscripts",
    "section": "",
    "text": "Some additional works not listed here can be found on Google Scholar and at HPCF Technical Reports. My ORCID is 0000-0002-4440-2330.\n\nManuscripts\nBibtex entries for the following are here.\n\nAndrew M. Raim, James A. Livsey, and Kyle M. Irimata (2025+). Rejection sampling with vertical weighted strips. [preprint].\nAndrew M. Raim, Kyle M. Irimata, and James A. Livsey (2025+). Self-Tuned Rejection Sampling within Gibbs and a Case Study in Small Area Estimation. [preprint].\nYi Huang, Karen Bandeen-Roche, Xiaoyu Dong, Andrew M. Raim, and Cunlin Wang (2024+). Latent propensity score approach for average causal effect estimation in observational studies with covariate measurement error. (In preparation).\n\n\n\nSelected Journal Articles\nBibtex entries for the following are here.\n\nAndrew M. Raim (2023). Direct sampling with a step function. Statistics and Computing, 33(1). [preprint] [doi].\nAndrew M. Raim, Thomas Mathew, Kimberly F. Sellers, Renee Ellis, and Mikelyn Meyers (2023). Design and sample size determination for experiments on nonresponse followup using a sequential regression model. Journal of Official Statistics, 39(2), 173-202. [preprint] [doi].\nAndrew M. Raim, Elizabeth Nichols, and Thomas Mathew (2023). A statistical comparison of call volume uniformity due to mailing strategy. Journal of Official Statistics, 39(1), 103-121. [preprint] [doi].\nRyan Janicki, Andrew M. Raim, Scott H. Holan, and Jerry J. Maples (2022). Bayesian nonparametric multivariate spatial mixture mixed effects models with application to American Community Survey special tabulations. The Annals of Applied Statistics, 16(1), 144-168. [preprint] [doi].\nAndrew M. Raim, Scott H. Holan, Jonathan R. Bradley, and Christopher K. Wikle (2021). Spatio-temporal change of support modeling with R. Computational Statistics, 36(1), 749-780. [preprint] [doi].\nSean Martin, Andrew Raim, Wen Huang, and Kofi Adragni (2020). ManifoldOptim: An R interface to the ROPTLIB library for Riemannian manifold optimization. Journal of Statistical Software, 93(1), 1-32. [doi].\nDarcy Steeg Morris, Andrew M. Raim, and Kimberly F. Sellers (2020). A Conway-Maxwell-multinomial distribution for flexible modeling of clustered categorical data. Journal of Multivariate Analysis, 179, 104651. [preprint] [doi].\nSai K. Popuri, Andrew M. Raim, Nagaraj K. Neerchal, and Matthias K. Gobbert (2018). Parallelizing computation of expected values in recombinant binomial trees. Journal of Statistical Computation and Simulation, 88(4), 657-674. [preprint] [doi].\nAndrew M. Raim, Nagaraj K. Neerchal, and Jorge G. Morel (2018). An extension of generalized linear models to finite mixture outcome distributions. Journal of Computational and Graphical Statistics, 27(3), 587-601. [preprint] [doi].\nAndrew M. Raim, Nagaraj K. Neerchal, and Jorge G. Morel (2017). An approximation to the information matrix of exponential family finite mixtures. Annals of the Institute of Statistical Mathematics, 69(2), 333-364. [preprint] [doi].\nDerek S. Young, Andrew M. Raim, and Nancy R. Johnson (2017). Zero-inflated modelling for characterizing coverage errors of extracts from the US Census Bureau’s Master Address File. Journal of the Royal Statistical Society: Series A, 180(1), 73-97. [doi].\nKofi P. Adragni, Elias Al-Najjar, Sean Martin, Sai K. Popuri, and Andrew M. Raim (2016). Group-wise sufficient dimension reduction with principal fitted components. Computational Statistics, 31(3), 923-941. [doi].\nKimberly F. Sellers and Andrew Raim (2016). A flexible zero-inflated model to address data dispersion. Computational Statistics and Data Analysis, 99, 68-80. [preprint] [doi].\nKofi Placid Adragni and Andrew M. Raim (2014). ldr: An R software package for likelihood-based sufficient dimension reduction. Journal of Statistical Software, 61(3). [doi].\nAndrew M. Raim, Minglei Liu, Nagaraj K. Neerchal, and Jorge G. Morel (2014). On the method of approximate Fisher scoring for finite mixtures of multinomials. Statistical Methodology, 18, 115-130. [preprint] [doi].\nAndrew M. Raim, Matthias K. Gobbert, Nagaraj K. Neerchal, and Jorge G. Morel (2013). Maximum-likelihood estimation of the random-clumped multinomial model as a prototype problem for large-scale statistical computing. Journal of Statistical Computation and Simulation, 83(12), 2178-2194. [preprint] [doi].\n\n\n\nSelected Reports, Proceedings, and Others\nBibtex entries for the following are here.\n\nAndrew M. Raim (2025). vws: Vertical Weighted Strips in R using C++. Research Report Series: Computing #2025-01. Center for Statistical Research and Methodology, U.S. Census Bureau. [www].\nMikelyn Meyers, Renee Ellis, Andrew Raim, Patricia Goerman, Kathleen Kephart, Kim Aspinwall, Patricia LeBaron, and Emilia Peytcheva (2025). 2020 census experiment: Spanish-speaking enumerator training. Report Series: Census Evaluations and Experiments. [www].\nAndrew M. Raim (2024). fntl: Numerical Tools for Rcpp and Lambda Functions. Research Report Series: Computing #2024-01. Center for Statistical Research and Methodology, U.S. Census Bureau. [www].\nAndrew M. Raim, Renee Ellis, & Mikelyn Meyers. (2024). A multinomial analysis of bilingual training and nonresponse followup contact rates in the 2020 Decennial Census. Study Series: Statistics #2024-01. Center for Statistical Research and Methodology, U.S. Census Bureau. [www].\nDarcy Steeg Morris and Andrew M. Raim (2023). Comparing trial and variable association in contingency table data using multinomial models for clustered data. In Elisabeth Bergherr, Andreas Groll, and Andreas Mayr, editors, 37th International Workshop on Statistical Modelling, pages 536-542. [www].\nAndrew M. Raim and Elizabeth Nichols (2023). A comparison of map usability via bivariate ordinal analysis. Study Series: Statistics #2023-01. Center for Statistical Research and Methodology, U.S. Census Bureau. [www].\nAndrew M. Raim and Kimberly F. Sellers (2022). COMPoissonReg: Usage, the normalizing constant, and other computational details. Research Report Series: Computing #2022-01. Center for Statistical Research and Methodology, U.S. Census Bureau. [www].\nKyle M. Irimata, Andrew M. Raim, Ryan Janicki, James A. Livsey, and Scott H. Holan (2022). Evaluation of Bayesian hierarchical models of differentially private data based on an approximate data model. Research Report Series: Statistics #2022-05. Center for Statistical Research and Methodology, U.S. Census Bureau. [www].\nAndrew M. Raim, James A. Livsey, and Kyle M. Irimata (2022). Browsing the 2010 Census SF2 summary file with R. Study Series: Computing #2022-01. Center for Statistical Research and Methodology, U.S. Census Bureau. [www].\nAndrew M. Raim (2021). Direct sampling in Bayesian regression models with additive disclosure avoidance noise. Research Report Series: Statistics #2021-01. Center for Statistical Research and Methodology, U.S. Census Bureau. [www].\nElizabeth Nichols, Erica Olmsted-Hawala, Andrew Raim, and Lin Wang (2020). Attitudinal and behavioral differences between older and younger adults using mobile devices. In Human aspects of IT for the aged population. Technologies, design and user experience. Springer Nature Switzerland AG. [doi].\nElizabeth Nichols, Sarah Konya, Rachel Horwitz, and Andrew Raim (2019). 2020 census research and testing report: The effect of the mail delivery date on survey login rates and helpline call rates. U.S. Census Bureau, Research and Methodology Directorate, Center for Behavioral Science Methods Research Report Series (Survey Methodology) #2019-01. U.S. Census Bureau. [www].\nDarcy Steeg Morris, Andrew M. Raim, and Kimberly F. Sellers (2018). Introducing a Conway-Maxwell-multinomial distribution for flexible modeling of categorical data. In JSM Proceedings, Biometrics Section, pages 716-733, Alexandria, VA. American Statistical Association. [preprint].\nAndrew M. Raim, Scott H. Holan, Jonathan R. Bradley, and Christopher K. Wikle (2017). A model selection study for spatio-temporal change of support. In JSM Proceedings, Government Statistics Section, pages 1524-1540, Alexandria, VA. American Statistical Association. [preprint].\nKrista Heim and Andrew M. Raim (2016). Predicting coverage error on the Master Address File using spatial modeling methods at the block level. In JSM Proceedings, Survey Research Methods Section, pages 1541-1555, Alexandria, VA. American Statistical Association. [preprint].\nAndrew M. Raim (2016). Informing maintenance to the U.S. Census Bureau’s Master Address File with statistical decision theory. In JSM Proceedings, Government Statistics Section, pages 648-659, Alexandria, VA. American Statistical Association. [preprint].\nAndrew M. Raim and Marissa N. Gargano (2015). Selection of predictors to model coverage errors in the Master Address File. Research Report Series: Statistics #2015-04. Center for Statistical Research and Methodology, U.S. Census Bureau. [www].\nAndrew M. Raim, Marissa N. Gargano, Nagaraj K. Neerchal, and Jorge G. Morel (2015). Bayesian analysis of overdispersed binomial data using mixture link regression. In JSM Proceedings, Statistical Computing Section, pages 2794-2808, Alexandria, VA, 2015. American Statistical Association. [preprint].\nAndrew M. Raim, Nagaraj K. Neerchal, and Jorge G. Morel (2015). Modeling overdispersion in R. Technical Report HPCF-2015-1. UMBC High Performance Computing Facility, University of Maryland, Baltimore County. [preprint] [github].\nAndrew M. Raim (2014). Computational methods in finite mixtures using approximate information and regression linked to the mixture mean. Ph.D. Thesis, Department of Mathematics and Statistics, University of Maryland, Baltimore County. [pdf] [www].\nAndrew M. Raim, Nagaraj K. Neerchal, and Jorge G. Morel (2014). Large cluster approximation to the finite mixture information matrix with an application to meta-analysis. In JSM Proceedings, Statistical Computing Section, pages 4025-4037, Alexandria, VA. American Statistical Association. [preprint].\nAndrew M. Raim (2013). Introduction to distributed computing with pbdR at the UMBC High Performance Computing Facility.Technical Report HPCF-2013-2. UMBC High Performance Computing Facility, University of Maryland, Baltimore County. [files] [www].\nAndrew M. Raim and Nagaraj K. Neerchal (2013). Modeling overdispersion in binomial data with regression linked to a finite mixture probability of success. In JSM Proceedings, Statistical Computing Section, pages 2760-2774, Alexandria, VA. American Statistical Association. [preprint].\nAndrew M. Raim, Brandon E. Fleming, and Nagaraj K. Neerchal (2012). An analysis of categorical injury data using mixtures of multinomials. In JSM Proceedings, Statistical Computing Section, pages 2444-2458, Alexandria, VA. American Statistical Association. [preprint].\nAndrew M. Raim and Matthias K. Gobbert (2010). Parallel performance studies for an elliptic test problem on the cluster tara. Technical Report HPCF-2010-2. UMBC High Performance Computing Facility, University of Maryland, Baltimore County. [preprint]."
  },
  {
    "objectID": "software/latex-business-card.html",
    "href": "software/latex-business-card.html",
    "title": "LaTeX Business Card",
    "section": "",
    "text": "There are a few nice tutorials on the web on how to make a business card in LaTeX. I started from this one and transformed it to a UMBC theme. I didn’t design this UMBC theme myself; I used a standard UMBC business card that I received, and tried to emulate it as closely as possible.\nNote that UMBC people can get their cards printed out at Commonvision and that students get a discount.\nHere is an example that uses the template. To generate it, download the following files and place them together in the same directory.\n\noffsetred.pdf a UMBC logo (from here), converted to PDF format.\nexample.tex latex for example.\n\nTo generate the PDF on a standard Linux setup:\n$ pdflatex example.tex\nAs usual, you may need to repeat the pdflatex command several times to resolve references in the document. This should yield the PDF file example.pdf You should also be able to compile the example with your favorite LaTeX IDE."
  },
  {
    "objectID": "software/latex-resume.html",
    "href": "software/latex-resume.html",
    "title": "LaTeX Resume / CV",
    "section": "",
    "text": "Here is a template for a resume or CV in LaTeX. It is especially useful if you maintain publications and other outputs in bibtex form. This template was originally based on Andrew McNabb’s template and customized to my own tastes. It was changed in a few ways:\n\nThe format is more suitable for a CV or long resume, rather than a one-page resume of bullet points.\nThe style definitions are separated from the main tex file (which contains the content of the resume/cv only).\n\nHere is an example that uses the template. You will probably find that some of the included macros are not relevant to you, or that something you need is missing. For example, a professor probably will not list their past TA assignments, but may want to list grants, graduate students, courses taught, etc. Feel free to modify the template for your own needs, aesthetics, etc.\nTo generate the example, download the following files and place them together in the same directory.\n\nresume.sty the resume style file\nexample.tex an example resume\npapers.bib example bibtex file containing some papers\npresentations.bib example bibtex file containing some presentations\n\nTo generate the PDF on a standard Linux setup:\n$ pdflatex example.tex\n$ bibtex bu1\n$ bibtex bu2\n$ pdflatex example.tex\nThe bibtex runs for for bu1 and bu2 come from the use of the bibunits package, which allow you to keep several separate bibliographies in one document. In the example, we have one bibliography for papers and one for presentations.\nAs usual, you may need to repeat the pdflatex command several times to resolve references in the document. This should yield the PDF file example.pdf You should also be able to compile the example with your favorite LaTeX IDE."
  },
  {
    "objectID": "software/formula-sheet.html",
    "href": "software/formula-sheet.html",
    "title": "LaTeX Formula Sheet",
    "section": "",
    "text": "This is a template for a formula sheet in LaTeX. Here is an example that uses the template. To generate it, download the following files and place them together in the same directory.\n\nformula-sheet.cls the class\nexample.tex the example\n\nTo generate the PDF on a standard Linux setup:\n$ pdflatex example.tex\nAs usual, you may need to repeat the pdflatex command several times to resolve references in the document. This should yield the PDF file example.pdf. You should also be able to compile the example with your favorite LaTeX IDE."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Introduction",
    "section": "",
    "text": "This page catalogues software projects and other useful resources on the web, especially those that I have personally found to be useful or interesting.\nMuch of the software listed here is free and open source with major contributions from - or developed entirely by - dedicated community members. These efforts have resulted in a vast selection of reliable, high quality, and readily-available software."
  },
  {
    "objectID": "resources.html#linux",
    "href": "resources.html#linux",
    "title": "Introduction",
    "section": "6.1 Linux",
    "text": "6.1 Linux\nLinux is an operating system (OS) started as a personal project by Linus Torvalds, who also created Git. An open source approach was taken in its development, allowing contributions from collaborators around the world, to become a viable alternative to corporate OS such as Microsoft Windows, Mac OS X, and Unix which was developed at Bell Labs. Linux is bundled in various distributions (distros), which typically include a desktop environment, service manager, package manager, and prepackaged software. Users can select a distro which best meets their needs. The Linux kernel is the basis of all Linux distros.\nFor Windows and Mac users looking to try Linux, I recommend starting with a mainstream Linux distro such as Debian or Ubuntu that includes a desktop environment (e.g., Xfce, KDE, or Gnome). This should provide a usable OS upon successful installation. There are several possible installation procedures to consider. To try Linux without making permanent changes to your computer, you can create a live USB and boot into it; this environment resets each time it is booted. A more persistent option is to install Linux to a virtual machine. When ready to set up Linux to boot as the computer’s actual OS, most installers include tools to set up dual boot so you can select between Linux and your original OS at boot time. If you happen to have an older PC or laptop lying around, it can serve as an excellent platform for trying Linux.\n\nDebian A community-maintained Linux distro. Distros such as Ubuntu, Linux Mint are based on Debian. Debian may be prefered over Ubuntu, for example, if you prefer not to use Snap, to start with a more minimal initial installation, or to use something maintained by a community rather than a company.\nArch Linux A community-maintained Linux distro that follows a rolling release model. Good for keeping up with the most recent versions of packages. Also good for customization because the initial installation is minimal. Not ideal for new Linux users because the installation process involves some command line work.\nLinux The Linux kernel."
  },
  {
    "objectID": "resources.html#window-managers-and-tools",
    "href": "resources.html#window-managers-and-tools",
    "title": "Introduction",
    "section": "6.2 Window Managers and Tools",
    "text": "6.2 Window Managers and Tools\nWindow managers (WMs) are components of desktop environments (DEs) that control how windows are displayed and how users navigate them. DEs such as Gnome, KDE, and XFCE in Linux include window managers and suites of graphical applications. Operating systems such as Microsoft Windows and Mac OS X include proprietary DEs. In some operating systems (especially Linux) it is possible to select your own WM and set of graphical applications for a completely customized experience.\nThere are two major windowing systems for Linux: X11 and Wayland. X11 has been the standard for many years, while Wayland is maturing in recent years and becoming more widely adopted. There is an X11 compatibility layer in Wayland, but window managers generally seem to be intended for one or the other.\n\ndwm A suckless tiling window manager for X11. Has inspired - or been used as a basis for - a number of other tiling window managers.\nslstatus A suckless status bar for X11. Pairs well with dwm for a suckless DIY setup.\nslock An extremely minimal suckless screen locking tool for X11.\nst The suckless terminal emulator for X11. Minimal and light on memory use but can be patched into a full-featured terminal.\ndmenu A suckless X11 menu tool. Can readily be leveraged in scripts to create user defined menus. Menus can be quickly traversed using a fuzzy search. Something similar can be accomplished on the command line with fzf.\nAwesome WM A window manager for X11 started as a fork of dwm that is non-suckless. Includes many features in the main code base and is highly configurable and extensible via Lua scripting.\nOXWM A window manager for X11 inspired by dwm which is written in Rust. Like Awesome WM, it non-suckless and configured via Lua. OXWM seems geared toward preserving the minimalist tiling experience of dwm, in contrast to Awesome, but includes sensible functionality that would require patches in dwm.\nXmonad Another non-suckless window manager for X11 which was inspired by dwm. Highly regarded in the online Linux community. Is written in the functional language Haskell; configuration is also done via Haskell.\ndwl A port of dwm for Wayland.\nNiri A scrolling window manager for Wayland that supports some aspects of tiling. I find this one to be very natural and intuitive. Here, each workspace is a row of windows where the windows may be spawned and removed as needed. One or more columns of the workspace can be on screen at a time. Users can navigate within and between workspaces, and dynamically rearrange the layout as desired.\nfoot A lightweight but full-featured terminal emulator for Wayland. Can be run as a daemon with clients to save resources when using multiple instances.\nrofi A more sophisticated menu tool with support for both X11 and Wayland. Features a “dmenu emulation” mode allowing it to be used as a drop-in replacement for dmenu.\nwaybar A highly customizable status bar for use with Wayland window managers.\nAwesome Wayland A curated list of tools for Wayland.\nXfce A desktop enviroment that is fully featured and intuitive while relatively light on system resource. Is visually pleasing while avoiding unnecessary graphical effects."
  },
  {
    "objectID": "resources.html#graphical-programs",
    "href": "resources.html#graphical-programs",
    "title": "Introduction",
    "section": "6.3 Graphical Programs",
    "text": "6.3 Graphical Programs\n\nFirefox A popular open source web browser.\nChromium The open source web browser on which Google Chrome is based.\nqutebrowser A minimalist open source web browser which is primary controlled by the keyboard. Uses Vim-style keybindings.\nVirtual Machine Manager An application to run and manage virtual machines. Uses the libvirt virtualization API. Both of these are open source.\nOBS Studio Capture your PC’s display, audio, and video for recording or streaming.\nAudacity Audio recording, editing, and processing.\nFlowblade Video editing.\nGIMP Image editing.\nVLC a media player for audio and video.\nXournalpp Take handwritten notes on your computer which can be edited and maintained as files. Especially useful with a writing tablet such as the Wacom Intuous.\nWrite Another excellent program for handwritten notes by Stylus Labs. Appears to be free but not completely open source like Xournal.\nDia Open source tool for drawing diagrams."
  },
  {
    "objectID": "resources.html#terminal-programs",
    "href": "resources.html#terminal-programs",
    "title": "Introduction",
    "section": "6.4 Terminal Programs",
    "text": "6.4 Terminal Programs\n\nCygwin A collection of open source tools that provide a Linux-like experience in Windows, especially through the command line.\ntmux Persistent terminal sessions that can be detached/attached and split into multiple panes. Here is a cheat sheet with some useful commands and keybindings.\nscreen An alternative to tmux; also provides persistent terminal sessions.\nVim Visual text editor for the terminal. Does not require a graphical environment. It is a “modal” editor because usage involves switching between several modes. There is a learning curve to become comfortable with basic use and then incorporate more advanced capabilities. Here is a browsable version of the internal quick reference. Here is a more condensed cheat sheet that is suitable for printing and hanging on a wall. There is also a universe of plugins to add functionality and customize the look and feel. Some favorite plugins are vimwiki to use Vim as a personal Wiki using Markdown (or several other syntax options) and lightline to give the status bar a sleek appearance. There are also language-specific plugins such as R.nvim for R and julia-vim for Julia.\nEmacs The traditional alternative to Vim which is just as extensive has has an equally dedicated user base. Emacs has a different feel which makes use of chord-style commands.\nNano A user-friendly text editor for the terminal. Recommended over Vim and Emacs for new terminal users.\nBash One of the standard shells for Linux-based systems. Supports scripting which can be used to automate tedious tasks.\nMutt A well-established email client for the terminal. Also see NeoMutt which is a fork of Mutt that supports some additional features.\nAlpine The successor of Pine which was another widely used email client for the terminal."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrew M. Raim",
    "section": "",
    "text": "Welcome! This website hosts my publicly released work.\nI have been a statistician at the U.S. Census Bureau Center for Statistical Research and Methodology (CSRM) since 2014. I was previously a graduate student at University of Maryland, Baltimore County (UMBC) and worked as a software engineer for the first part of my career at the internet advertising company Advertising.com.\nThe best way to contact me is to send email to andrew dot raim at gmail dot com.\n \n  \n   \n  \n    \n     CV\n  \n  \n    \n     Github\n  \n  \n    \n     Bitbucket\n  \n  \n    \n     ORCID\n  \n  \n    \n     LinkedIn"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Andrew M. Raim",
    "section": "",
    "text": "vws: Vertical Weighted Strips in R using C++\n\n\n\narticle\n\nprogramming\n\n\n\n\n\n\nNov 5, 2025\n\n\n\n\n\n\n\nSelf-Tuned VWS within Gibbs and Small Area Estimation\n\n\n\narticle\n\n\n\n\n\n\nSep 23, 2025\n\n\n\n\n\n\n\nNumerical Maximum Likelihood with the stats4 Package\n\n\n\nprogramming\n\n\n\n\n\n\nDec 11, 2024\n\n\n\n\n\n\n\nfntl: Numerical Tools for Rcpp and Lambda Functions\n\n\n\narticle\n\nprogramming\n\n\n\n\n\n\nJul 16, 2024\n\n\n\n\n\n\n\nRejection Sampling with Vertical Weighted Strips\n\n\n\narticle\n\n\n\n\n\n\nJan 19, 2024\n\n\n\n\n\n\n\nSending Mail from R\n\n\n\nprogramming\n\n\n\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\nComparison of Usability Preferences with Bivariate Ordinal Regression\n\n\n\narticle\n\n\n\n\n\n\nApr 20, 2023\n\n\n\n\n\n\n\nInteractive Map with Shiny and Leaflet\n\n\n\nprogramming\n\n\n\n\n\n\nFeb 27, 2023\n\n\n\n\n\n\n\nWorker Tool for Computational Studies\n\n\n\nprogramming\n\n\n\n\n\n\nFeb 12, 2023\n\n\n\n\n\n\n\nGenerating Latex Tables in R\n\n\n\nprogramming\n\n\n\n\n\n\nFeb 11, 2023\n\n\n\n\n\n\n\nCOMPoissonReg: Usage, the Normalizing Constant, and Other Computational Details\n\n\n\narticle\n\nprogramming\n\n\n\n\n\n\nNov 15, 2022\n\n\n\n\n\n\n\nBrowsing summary files from the decennial census with R\n\n\n\narticle\n\nprogramming\n\n\n\n\n\n\nAug 15, 2022\n\n\n\n\n\n\n\nknitr and Latex Documents\n\n\n\nprogramming\n\n\n\n\n\n\nJun 27, 2022\n\n\n\n\n\n\n\nPresentation for 2020 Joint Statistical Meetings\n\n\n\nconference\n\n\n\n\n\n\nJul 27, 2020\n\n\n\n\n\n\n\nR Workshop in Kerala\n\n\n\nworkshop\n\n\n\n\n\n\nJan 5, 2018\n\n\n\n\n\n\n\nRcpp Workshop at UMBC\n\n\n\nworkshop\n\n\n\n\n\n\nAug 7, 2017\n\n\n\n\n\n\n\nWeb Page Migration\n\n\n\nadmin\n\n\n\n\n\n\nMar 15, 2017\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "bib/articles.html",
    "href": "bib/articles.html",
    "title": "Andrew M. Raim",
    "section": "",
    "text": "@article{NRFUExperiments2023,\n  title = {Design and Sample Size Determination for Experiments on Nonresponse\n           Followup using a Sequential Regression Model},\n  author = {Andrew M. Raim and Thomas Mathew and Kimberly F. Sellers and Renee\n            Ellis and Mikelyn Meyers},\n  journal = {Journal of Official Statistics},\n  year = 2023,\n  volume = 39,\n  number = 2,\n  pages = {173--202},\n  doi = {doi:10.2478/jos-2023-0009},\n  url = {https://doi.org/10.2478/jos-2023-0009},\n  preprint = \"https://www.census.gov/library/working-papers/2020/adrm/RRS2020-03.html\",\n}\n\n@article{CallUniformity2023,\n  title = {A Statistical Comparison of Call Volume Uniformity Due to Mailing\n           Strategy},\n  author = {Andrew~M. Raim and Elizabeth Nichols and Thomas Mathew},\n  journal = {Journal of Official Statistics},\n  year = 2023,\n  volume = 39,\n  number = 1,\n  pages = {103--121},\n  preprint = \"https://www.census.gov/library/working-papers/2018/adrm/RRS2018-09.html\",\n  doi = {10.2478/jos-2023-0005},\n  url = {https://doi.org/10.2478/jos-2023-0005},\n}\n\n@article{DirectSamplingStep2023,\n  title = {Direct sampling with a step function},\n  author = {Raim, Andrew M.},\n  journal = {Statistics and Computing},\n  year = 2023,\n  volume = 33,\n  number = 1,\n  issn = {1573-1375},\n  doi = {10.1007/s11222-022-10188-x},\n  preprint = {https://arxiv.org/abs/2203.15852},\n}\n  url = {10.1007/s11222-022-10188-x},\n  pages = 22,\n\n@article{ACSST2022,\n  title = {{Bayesian nonparametric multivariate spatial mixture mixed effects\n            models with application to American Community Survey special\n            tabulations}},\n  author = {Ryan Janicki and Andrew~M. Raim and Scott~H. Holan and Jerry~J. Maples},\n  journal = {The Annals of Applied Statistics},\n  year = 2022,\n  volume = 16,\n  number = 1,\n  pages = {144--168},\n  preprint = \"https://arxiv.org/abs/2009.12351\",\n  primaryClass = {stat.AP},\n  doi = {10.1214/21-AOAS1494},\n  url = {https://dx.doi.org/10.1214/21-AOAS1494},\n}\n\n@article{STCOS2021,\n  title = \"Spatio-Temporal Change of Support Modeling with {R}\",\n  author = \"Andrew~M. Raim and Scott~H. Holan and Jonathan~R. Bradley\n            and Christopher~K. Wikle\",\n  journal = \"Computational Statistics\",\n  year = 2021,\n  volume = 36,\n  number = 1,\n  pages = \"749--780\",\n  preprint = {https://arxiv.org/abs/1904.12092},\n  primaryClass = {stat.CO},\n  doi = \"10.1007/s00180-020-01029-4\",\n  url = \"https://dx.doi.org/10.1007/s00180-020-01029-4\",\n}\n\n@article{CMM2020,\n  title = \"A {C}onway-{M}axwell-multinomial distribution for flexible\n           modeling of clustered categorical data\",\n  author = \"Darcy Steeg Morris and Andrew M. Raim and Kimberly F. Sellers\",\n  journal = \"Journal of Multivariate Analysis\",\n  year = \"2020\",\n  volume = 179,\n  pages = \"104651\",\n  issn = \"0047-259X\",\n  doi = \"10.1016/j.jmva.2020.104651\",\n  url = \"https://www.sciencedirect.com/science/article/pii/S0047259X20302323\",\n  preprint = {https://arxiv.org/abs/1911.02131},\n  primaryClass = {stat.ME},\n}\n\n@article{ManifoldOptim2020,\n  title = \"{ManifoldOptim}: An {R} Interface to the {ROPTLIB} Library for\n           {R}iemannian Manifold Optimization\",\n  author = {Sean Martin and Andrew Raim and Wen Huang and Kofi Adragni},\n  journal = {Journal of Statistical Software},\n  year = 2020,\n  volume = 93,\n  number = 1,\n  issn = {1548-7660},\n  pages = {1--32},\n  primaryClass = {stat.CO},\n  preprint = {arXiv:1612.03930},\n  doi = {10.18637/jss.v093.i01},\n  url = {https://www.jstatsoft.org/v093/i01},\n}\n\n@article{PopuriEtAlBinomial2018,\n  author = {Sai~K. Popuri and Andrew~M. Raim and Nagaraj~K. Neerchal and\n            Matthias~K. Gobbert},\n  title = {Parallelizing computation of expected values in recombinant\n           binomial trees},\n  journal = {Journal of Statistical Computation and Simulation},\n  year  = 2018,\n  volume = 88,\n  number = 4,\n  pages = {657--674},\n  primaryClass = {stat.CO},\n  preprint = {https://arxiv.org/abs/1701.03512},\n  doi = {10.1080/00949655.2017.1402898},\n  url = {https://dx.doi.org/10.1080/00949655.2017.1402898},\n}\n\n@article{MixLinkGLM2018,\n  title = \"An Extension of Generalized Linear Models to Finite Mixture Outcome\n           Distributions\",\n  author = \"Andrew~M. Raim and Nagaraj~K. Neerchal and Jorge~G. Morel\",\n  journal = \"Journal of Computational and Graphical Statistics\",\n  year = 2018,\n  volume = 27,\n  number = 3,\n  pages = \"587--601\",\n  primaryClass = {stat.ME},\n  preprint = {https://arxiv.org/abs/1612.03302},\n  doi = \"10.1080/10618600.2017.1391698\",\n  url = \"https://dx.doi.org/10.1080/10618600.2017.1391698\",\n}\n\n@article{RaimEtAlAfimExpFam2017,\n  title = \"An Approximation to the Information Matrix of Exponential Family\n           Finite Mixtures\",\n  author = \"Andrew~M. Raim and Nagaraj~K. Neerchal and Jorge~G. Morel\",\n  journal = \"Annals of the Institute of Statistical Mathematics\",\n  pages = \"333--364\",\n  volume = 69,\n  number = 2,\n  year = 2017,\n  doi = \"10.1007/s10463-015-0542-9\",\n  url = \"https://dx.doi.org/10.1007/s10463-015-0542-9\",\n  preprint = {https://andrewraim.github.io/downloads/doc/RaimEtAlAfimExpFam2014.pdf},\n}\n\n@article{YoungRaimJohnsonMAF2017,\n  author = \"Young, Derek~S. and Raim, Andrew~M. and Johnson, Nancy~R.\",\n  title = \"Zero-inflated modelling for characterizing coverage errors of\n           extracts from the {US} {C}ensus {B}ureau's {M}aster {A}ddress\n           {F}ile\",\n  journal = \"Journal of the Royal Statistical Society: Series A\",\n  volume = 180,\n  number = 1,\n  pages = \"73--97\",\n  year = 2017,\n  url = \"https://dx.doi.org/10.1111/rssa.12183\",\n  doi = \"10.1111/rssa.12183\",\n}\n\n@article{SellersRaimZICMP2016,\n  title = \"A flexible zero-inflated model to address data dispersion\",\n  author = \"Kimberly F. Sellers and Andrew Raim\",\n  journal = \"Computational Statistics and Data Analysis\",\n  volume = 99,\n  pages = \"68--80\",\n  year = 2016,\n  doi = \"10.1016/j.csda.2016.01.007\",\n  url = \"https://www.sciencedirect.com/science/article/pii/S0167947316000165\",\n  preprint = \"https://faculty.georgetown.edu/kfs7/MY%20PUBLICATIONS/SellersAndRaim2016CSDA.pdf\",\n}\n\n@article{AdragniEtAlCOST2016,\n  title = \"Group-wise sufficient dimension reduction with principal fitted\n           components\",\n  author = \"Kofi~P. Adragni and Elias Al-Najjar and Sean Martin and\n            Sai~K. Popuri and Andrew~M. Raim\",\n  year = 2016,\n  journal = \"Computational Statistics\",\n  volume = \"31\",\n  number = \"3\",\n  pages = \"923--941\",\n  doi = \"10.1007/s00180-015-0611-9\",\n  url = \"https://dx.doi.org/10.1007/s00180-015-0611-9\",\n}\n\n@article{AdragniRaimLDR2014,\n  author =  \"Kofi Placid Adragni and Andrew M. Raim\",\n  title = \"{ldr}: An {R} Software Package for Likelihood-Based Sufficient\n             Dimension Reduction\",\n  journal = \"Journal of Statistical Software\",\n  volume = 61,\n  number = 3,\n  year = 2014,\n  URL = \"https://www.jstatsoft.org/v61/i03\",\n}\n\n@article{RaimEtAlStamet2014,\n  title = \"On the method of approximate {F}isher scoring for finite mixtures\n           of multinomials\",\n  author = \"Andrew~M. Raim and Minglei Liu and Nagaraj~K. Neerchal and\n            Jorge~G. Morel\",\n  journal = \"Statistical Methodology\",\n  volume = \"18\",\n  pages = \"115--130\",\n  year = \"2014\",\n  note = \"\",\n  issn = \"1572-3127\",\n  preprint = {https://andrewraim.github.io/downloads/doc/RaimEtAlStamet2014.pdf},\n  url = \"https://www.sciencedirect.com/science/article/pii/S1572312713000841\",\n  doi = \"10.1016/j.stamet.2013.10.002\",\n}\n\n@article{RaimEtAlJSCS2013,\n  author = \"Raim, Andrew~M. and Gobbert, Matthias~K. and Neerchal, Nagaraj~K.\n            and Morel, Jorge~G.\",\n  title = \"Maximum-likelihood Estimation of the Random-Clumped Multinomial\n           Model as a Prototype Problem for Large-Scale Statistical Computing\",\n  journal = \"Journal of Statistical Computation and Simulation\",\n  volume = 83,\n  number = 12,\n  pages = \"2178--2194\",\n  year = 2013,\n  url = \"http://www.tandfonline.com/doi/abs/10.1080/00949655.2012.684095\",\n  preprint = \"https://userpages.umbc.edu/~gobbert/papers/RaimEtAlJSCS2010.pdf\",\n  doi = \"10.1080/00949655.2012.684095\",\n}"
  },
  {
    "objectID": "bib/manuscripts.html",
    "href": "bib/manuscripts.html",
    "title": "Andrew M. Raim",
    "section": "",
    "text": "@unpublished{VWS2024+,\n  title = \"Rejection Sampling with Vertical Weighted Strips\",\n  author = \"Andrew~M. Raim and James~A. Livsey and Kyle~M. Irimata\",\n  year = \"2024+\",\n  url = \"https://arxiv.org/abs/2401.09696\",\n}\n\n@unpublished{VWSR2025,\n  title = \"{vws}: Vertical Weighted Strips in R using C++\",\n  author = \"Andrew~M. Raim\",\n  year = \"2025+\",\n  note = \"In Preparation\",\n}\n\n@unpublished{SAEVWS2025+,\n  title = \"Self-Tuned Rejection Sampling within {G}ibbs and a Case Study in\n           Small Area Estimation\",\n  author = \"Andrew~M. Raim and Kyle~M. Irimata and James~A. Livsey\",\n  year = \"2025+\",\n  note = \"Submitted\",\n  preprint = \"https://arxiv.org/abs/2509.17155\",\n}"
  },
  {
    "objectID": "posts/2023-02-12-worker.html",
    "href": "posts/2023-02-12-worker.html",
    "title": "Worker Tool for Computational Studies",
    "section": "",
    "text": "Large statistical simulations and other computational studies often consist of a collection of independent tasks to be processed. For example, to study properties of an estimator, we may wish to vary the parameters of an assumed data-generating mechanism along with the sample size. The estimator requires substantial amount of computation because it makes use of a Markov chain Monte Carlo (MCMC) procedure. We generate 500 datasets in each setting, evaluate the estimator, and save the results to be summarized and later. From a parallel computing perspective, such tasks are described as embarrassingly parallel because they do not need to interact while they run.\nThere are usually many more tasks than available processors. Suppose we are limited to the use of \\(k\\) processors; we would like to keep these \\(k\\) processors busy with our workload without using additional processors. Ideally, we would like to do this without the need for manual intervention. Worker is a lightweight tool to help automate this process.\nSuppose our study consists of a one parameter with 1000 levels. Suppose that we have set up 1000 corresponding directories, named as follows.\n$ ls -1\nrun000\nrun001\n...\nrun999\nIn each folder, suppose there is a script launch.R that runs the corresponding level of the simulation. An output file output.csv is placed in the directory upon successful completion of launch.R.\n$ ls run000\nlaunch.R    output.csv\nWe would like to run \\(k\\) of the launch.R scripts at a time until the collection of 1000 tasks is complete. The worker script attempts to automate this in a way that is agnostic of the underlying simulation. It loops through tasks and runs them sequentially, but multiple workers can be run simultaneously on the same study to achieve “embarrassingly parallel” parallel computing. Workers make use of lock files to avoid stepping on each other’s toes: each task is taken up by only one worker. This paradigm can be useful in shared computing environments where users may be asked to limit the number of processors requested at any given time. We can start, say, ten workers to ensure ten simultaneous tasks rather than having to continually monitor a scheduler’s queue and submit more tasks.\nHere is an example of the syntax used to invoke each worker in this example.\n$ worker.sh -p 'run???' -c 'Rscript launch.R'\nThe worker looks for folders whose names match pattern run???. When such a folder is found, it enters the folder and first checks to see if the task has previously been claimed by a worker (including itself in a previous iteration). If not, it claims the task and launches it via the specified command Rscript launch.R. The task is run in the worker’s foreground until completion. The worker terminates when it is no longer actively engaged and there are no unclaimed jobs remaining.\nAfter all of the runs are completed, we will likely want to load each of the output.csv files and construct tables and plots to summarize the results. This post may be helpful if you are looking to construct Latex tables from a computational study."
  },
  {
    "objectID": "posts/2017-08-07-umbc-workshop-Rcpp.html",
    "href": "posts/2017-08-07-umbc-workshop-Rcpp.html",
    "title": "Rcpp Workshop at UMBC",
    "section": "",
    "text": "Note\n\n\n\nPlease note that the workshop date has been moved from Friday Sept 8, 2017 to Friday Sept 22, 2017. See below for details.\n\n\nIris Gauran and I are planning to give a half-day workshop on Rcpp at UMBC. Tentative details are given below. Part of the workshop will be a “quick start”, where we will demonstrate accessing C++ code from R via toy examples. Attendees are encouraged to prepare their laptops for Rcpp programming, and bring them to the workshop to follow along with this portion.\nWe will focus on Rcpp programming in RStudio, which provides a common interface across all major platforms (Windows, Mac, and Linux), and helps to automate some procedural tasks.\n\nInstall R (http://www.r-project.org). The current version as of this writing is 3.4.1 “Single Candle”.\nInstall RStudio Desktop (http://www.rstudio.com). The current version is 1.0.153.\nInstall the Rcpp, RcppArmadillo, and RcppGSL packages.\nSome additional libraries may be needed to compile C++ programs on your computer. These differ depending on your operating system. See http://support.rstudio.com/hc/en-us/articles/200486498-Package-Development-Prerequisites.\n\nThis may be a good time to upgrade if you have older versions of R, RStudio, or any of the listed packages.\nTo test your Rcpp environment, try the following in RStudio.\n\nSelect File =&gt; New File =&gt; C++ File from the main menu. This will create a simple Rcpp program.\nSave the program, say, as example.cpp.\nClick the Source button, which is located in the upper-right corner of the panel with the example.cpp source code. This should produce the output\n\nR&gt; timesTwo(42)\n[1] 84\n\nLocation: University of Maryland, Baltimore County.\nRoom: MP 401 (Math/Stat Seminar Room).\nTime: Friday Sept 22, 1 - 4:30pm.\nCost: This workshop is free for the UMBC community.\nPrerequisites: intermediate knowledge in a high level language like R, Python, or Matlab. Familiarity with R and RStudio is highly recommended.\nSnacks: Light refreshments (coffee and bagels) will be provided.\nAbstract\nR is the preferred computing environment for many statisticians, and is used both in research and applied problems. R has achieved tremendous popularity because it is free, open source, and available on all modern platforms (Windows, Mac, and Linux). The R programming language is simple and intuitive, and well-suited for fast prototyping of complicated algorithms. However, R users can often find the performance of their programs to be lacking.\nThis workshop will demonstrate Rcpp, an extension of R that facilitates interoperability between R and C++ (Eddelbuettel, 2013). With Rcpp, computationally intensive parts of your program can be written in C++ and seamlessly called from R, potentially giving dramatic performance improvements within the familiar R user environment. We will provide a quick start for new Rcpp users through simple examples, and also examine several larger-scale statistical applications. C++ programming is usually more burdensome than R programming, so we will discuss tradeoffs between optimizing R code and migrating to C++, and ways to minimize suffering while making the transition.\nReferences\n\nDirk Eddelbuettel. Seamless R and C++ Integration with Rcpp. Springer, 2013.\nAlso see http://www.rcpp.org."
  },
  {
    "objectID": "posts/2017-03-15-website.html",
    "href": "posts/2017-03-15-website.html",
    "title": "Web Page Migration",
    "section": "",
    "text": "I am now using GitHub Pages to host my website, along with the static site generator Jekyll. My previous website was hosted at UMBC."
  },
  {
    "objectID": "posts/2024-07-16-fntl.html",
    "href": "posts/2024-07-16-fntl.html",
    "title": "fntl: Numerical Tools for Rcpp and Lambda Functions",
    "section": "",
    "text": "Lambda functions were added to C++ in the C++11 specification. They are functions which can be defined on the fly in the course of a program, can make use of variables in the environment within their body, and be passed as arguments to other functions. Use of lambdas may be especially appealing for Rcpp programmers who are accustomed to working with function objects in R but occasionally need C++ when performance becomes a concern.\nThe objective of the R package fntl (Raim 2024a) is to facilitate programming in Rcpp with lambda functions by providing an API to routinely needed numerical tools such as integration, root-finding, and optimization. Such functions require one or more functions as a primary argument; these are supplied as lambdas to fntl. Where possible, the fntl API utilizes methods exposed from the R API so that the same numerical methods are used as in R. In cases where methods are not exposed from the R API, fntl implements methods that are intended to be comparable. A detailed guide to the fntl API is provided in the package vignette (Raim 2024b).\nfntl package\n\nDeployed on CRAN\nSource on Github\n\n\n\n\n\nReferences\n\nRaim, Andrew M. 2024a. fntl: Numerical Tools for Rcpp and Lambda Functions. https://github.com/andrewraim/fntl.\n\n\n———. 2024b. “fntl: Numerical Tools for Rcpp and Lambda Functions.” Research Report Series: Computing #2024-01. Center for Statistical Research; Methodology, U.S. Census Bureau. https://www.census.gov/library/working-papers/2024/adrm/RRC2024-01.html."
  },
  {
    "objectID": "posts/2022-08-15-sfreader.html",
    "href": "posts/2022-08-15-sfreader.html",
    "title": "Browsing summary files from the decennial census with R",
    "section": "",
    "text": "If you have needed to use decennial census data without access to the internet, you may have encountered the raw data form of the summary files. These may be downloaded from the Census Bureau website for offline use; e.g., here. The format of the files is compact but can be difficult to navigate. James Livsey, Kyle Irimata, and I prepared a technical report and a package to assist R users with this via the tidyverse. These materials focus on the 2010 SF2 summary file, but support for others may be added in the future as needed."
  },
  {
    "objectID": "posts/2022-06-27-knitr-latex.html",
    "href": "posts/2022-06-27-knitr-latex.html",
    "title": "knitr and Latex Documents",
    "section": "",
    "text": "Rmarkdown seems to be the most standard way to embed R code and results into a document. Markdown has many benefits compared to Latex: it is much easier to get started with and the source code is closer to a plain readable text document. It is also possible to use Latex within Markdown when an occasional equation is needed. But sometimes you really want to work in Latex without going through Markdown.\nI have been aware that Sweave allows embedded R for Latex documents, but it seemed archaic compared to more modern R tools for reproducible research, and I avoided using it. Recently, I was made aware that Overleaf supports authoring Sweave documents which require very little special markup and are almost like working in regular Latex. After some additional searching, I found that this is also possible without Overleaf. knitr has long been able to do this.\nExamples of Sweave with knitr can be found online, but I thought it would be worthwhile to post several more. See the repo https://github.com/andrewraim/sweave-templates for an example in article format (pdf) and one in Beamer slide format (pdf)."
  },
  {
    "objectID": "posts/2023-04-20-bivariate-ordinal-maps.html",
    "href": "posts/2023-04-20-bivariate-ordinal-maps.html",
    "title": "Comparison of Usability Preferences with Bivariate Ordinal Regression",
    "section": "",
    "text": "Suppose \\(n\\) respondents are asked to rate several alternatives on a scale from, say, 1 to 5, with 1 being least preferred and 5 being most preferred. How can we determine whether one alternative is significantly more preferred to the others? The data may include covariates which are associated with the preference of interest.\nElizabeth Nichols and I carry out this kind of analysis on map usability in a recent report (Raim and Nichols 2023). With ordinal data recorded in a survey setting, we compare preferences for satellite maps - which display details such as landmarks and terrain - to those of simplified road maps. We make use of a bivariate ordinal regression model to carry out the analysis. Computations are carried out using the mvord R package by Hirk, Hornik, and Vana (2020), which can also support multivariate models with more than two alternatives.\n\n\n\n\nReferences\n\nHirk, Rainer, Kurt Hornik, and Laura Vana. 2020. “mvord: An R Package for Fitting Multivariate Ordinal Regression Models.” Journal of Statistical Software 93 (4): 1–41. https://doi.org/10.18637/jss.v093.i04.\n\n\nRaim, Andrew M., and Elizabeth Nichols. 2023. “A Comparison of Map Usability via Bivariate Ordinal Analysis.” Study Series: Statistics #2023-01. Center for Statistical Research; Methodology, U.S. Census Bureau. https://www.census.gov/library/working-papers/2023/adrm/SSS2023-01.html."
  },
  {
    "objectID": "posts/2017-12-20-kerala-workshop.html",
    "href": "posts/2017-12-20-kerala-workshop.html",
    "title": "R Workshop in Kerala",
    "section": "",
    "text": "George Ostrouchov, Nagaraj Neerchal and I are giving a two-day workshop High Performance Statistical Computing using R at the International Conference on Recent Advances in Statistical Methodology with Applications in Clinical and Official Statistics (ICSA 2018), Department of Statistics, St. Thomas College, Pala, Kerala, India. An abstract and brief outline for the workshop are given here.\nThe workshop builds up to parallel and distributed computing in R, but begins by introducing basics of working in R. Previous experience with R and Rstudio will be helpful, but much of the material should be accessible to new R users with experience in another technical computing language such as Matlab, Python, or Julia. Applications involving statistical and machine learning methods will be presented throughout the workshop. A bachelors degree in a discipline such as statistics, computer science, or mathematics - OR equivalent work experience - should be sufficient to understand the methodology.\nAttendees are encouraged to bring their laptops. Many example codes will be provided, and there will be opportunities at some points during the workshop for users to follow along. If you will be bringing your laptop, the remainder of this page discusses how to prepare it for the workshop.\n\nIt is possible to manually install R, Rstudio, and all packages that will be used in the workshop. But this is quite impractical, especially for the material on parallel and distributed programming which varies somewhat based on the computing platform (Windows, Mac, Linux). Therefore, we highly suggest a second option…\nInstead, we have prepared a Docker container, which is like a virtual machine. This will provide all attendees with the same programming environment which contains all the software needed for the workshop.\n\nInstructions to obtain and run the Docker container are given below. For attendees who cannot install Docker, alternative instructions are provided further below.\nWorkshop contents such as slides, programs, and dataset will be provided at the start of the workshop."
  },
  {
    "objectID": "posts/2017-12-20-kerala-workshop.html#windows",
    "href": "posts/2017-12-20-kerala-workshop.html#windows",
    "title": "R Workshop in Kerala",
    "section": "2.1 Windows",
    "text": "2.1 Windows\nNote that Windows users who do not have Windows 10 Pro or Enterprise - which will probably be most of us - will need a version of Docker called Docker Toolbox. Once it is installed, open the Docker Quickstart Terminal to issue commands, which should appear as follows.\n\n\n\nLogging into Rstudio Server\n\n\nTake note of the IP address listed in the output; we will use it later when connecting to Rstudio.\nI initially had some trouble installing Docker Toolbox on Windows 10 Home. I believe it was due to low disk space - I suggest having at least a 5-10 GB available before installing. For me, a solution was to remove Docker Toolbox, and the Oracle VirtualBox program installed with it, remove the .docker and .virtualbox subdirectories in C:\\Users\\Andrew, and rerun the Docker Toolbox installer. It may also be helpful to temporarily turn off Windows Firewall, or your antivirus program’s firewall if you have one, during the installation."
  },
  {
    "objectID": "posts/2017-12-20-kerala-workshop.html#linux",
    "href": "posts/2017-12-20-kerala-workshop.html#linux",
    "title": "R Workshop in Kerala",
    "section": "2.2 Linux",
    "text": "2.2 Linux\nLinux users should follow the installation instructions specific to their Linux distribution (Ubuntu, CentOS, etc). Once it is installed, Docker can be controlled through the system terminal.\nNote that Docker commands on Linux need administrator-level access, and must be prefixed with sudo. Because we anticipate mostly Windows users in attendance, the sudo has been left out of the commands shown on the remainder of this page."
  },
  {
    "objectID": "posts/2017-12-20-kerala-workshop.html#mac",
    "href": "posts/2017-12-20-kerala-workshop.html#mac",
    "title": "R Workshop in Kerala",
    "section": "2.3 Mac",
    "text": "2.3 Mac\nMac users should follow their set of installation instructions. As with Linux, once Docker is installed on a Mac, it can be controlled through the Terminal application. sudo is also needed for Mac when executing Docker commands."
  },
  {
    "objectID": "posts/2017-12-20-kerala-workshop.html#a-quick-test-of-your-docker-setup",
    "href": "posts/2017-12-20-kerala-workshop.html#a-quick-test-of-your-docker-setup",
    "title": "R Workshop in Kerala",
    "section": "2.4 A Quick Test of your Docker Setup",
    "text": "2.4 A Quick Test of your Docker Setup\nIn your terminal, issue the following command.\n$ docker run hello-world\nHere, $ represents the prompt, which is not part of the command. If your Docker setup is working, you should get the following result.\n$ docker run hello-world\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\nShare images, automate workflows, and more with a free Docker ID:\n https://cloud.docker.com/\nFor more examples and ideas, visit:\n https://docs.docker.com/engine/userguide/"
  },
  {
    "objectID": "posts/2017-12-20-kerala-workshop.html#r-on-the-command-line",
    "href": "posts/2017-12-20-kerala-workshop.html#r-on-the-command-line",
    "title": "R Workshop in Kerala",
    "section": "5.1 R on the Command Line",
    "text": "5.1 R on the Command Line\nTo start R on the command line, issue the R command.\nrstudio@a8f9a900c791:~$ R\n&gt; cat(\"Hello world\\n\")\nHello world"
  },
  {
    "objectID": "posts/2017-12-20-kerala-workshop.html#rstudio",
    "href": "posts/2017-12-20-kerala-workshop.html#rstudio",
    "title": "R Workshop in Kerala",
    "section": "5.2 Rstudio",
    "text": "5.2 Rstudio\nTo connect to Rstudio server, open a web browser on your laptop.\n\nMac and Linux users should navigate to the URL http://localhost:8787.\nWindows users should recall the IP address they noted back in section 2.1. Suppose your assigned IP was 192.168.99.100 (which is the one I have in the screenshot); navigate to the URL http://192.168.99.100:8787. If you forgot the IP address, start up another Docker Terminal and run the following.\n\n$ docker-machine ls\nA login prompt should appear in your browser. Enter rstudio as both the username and the password.\n\n\n\nLogging into Rstudio Server\n\n\nNow you should be ready to use Rstudio in your web browser.\n\n\n\nRstudio Server in browser"
  },
  {
    "objectID": "posts/2017-12-20-kerala-workshop.html#r-commander",
    "href": "posts/2017-12-20-kerala-workshop.html#r-commander",
    "title": "R Workshop in Kerala",
    "section": "5.3 R Commander",
    "text": "5.3 R Commander\nTo launch the R Commander GUI, first start R via the command line or Rstudio. Then issue the following command.\n&gt; library(Rcmdr)"
  },
  {
    "objectID": "posts/2017-12-20-kerala-workshop.html#running-mpi-jobs",
    "href": "posts/2017-12-20-kerala-workshop.html#running-mpi-jobs",
    "title": "R Workshop in Kerala",
    "section": "5.4 Running MPI Jobs",
    "text": "5.4 Running MPI Jobs\nTo demonstrate running an MPI job, let us use a simple Hello World example. Open a text editor on your laptop and save the following code to the file /path/to/workshop/hello.R\nlibrary(pbdMPI, quiet = TRUE)\n\nmsg &lt;- sprintf(\"Hello world from process %d\\n\", comm.rank(), comm.size())\ncomm.cat(\"Say hello:\\n\", quiet = TRUE)\ncomm.cat(msg, all.rank = TRUE)\n\nfinalize()\nRecall that this file will be accessible inside the container via the path /home/rstudio/ext/hello.R. Inside the container, you should be able to run the script in parallel via the mpirun command.\nrstudio@a8f9a900c791:~$ mpirun -np 4 Rscript ~/ext/hello.R\nSay hello:\nCOMM.RANK = 0\nHello world from process 0\nCOMM.RANK = 1\nCOMM.RANK = 2\nHello world from process 1\nHello world from process 2\nCOMM.RANK = 3\nHello world from process 3\nOutputs from the container should be saved to /home/rstudio/ext. This will allow you to view images, PDFs, etc using the tools already installed on your laptop.\nIf you made it this far, congratulations - your laptop is ready!"
  },
  {
    "objectID": "posts/2017-12-20-kerala-workshop.html#install-r",
    "href": "posts/2017-12-20-kerala-workshop.html#install-r",
    "title": "R Workshop in Kerala",
    "section": "7.1 Install R",
    "text": "7.1 Install R\nInstall R from CRAN. Windows users should navigate to “Download R for Windows” at the top of the page. We are using version 3.4.3."
  },
  {
    "objectID": "posts/2017-12-20-kerala-workshop.html#install-rstudio",
    "href": "posts/2017-12-20-kerala-workshop.html#install-rstudio",
    "title": "R Workshop in Kerala",
    "section": "7.2 Install Rstudio",
    "text": "7.2 Install Rstudio\nInstall Rstudio. Click Products =&gt; RStudio on the top menu, and look for a button labeled Download Rstudio Desktop. Select RStudio Desktop Open Source License among the available versions. We are using version 1.1.383."
  },
  {
    "objectID": "posts/2017-12-20-kerala-workshop.html#install-the-rcpp-package",
    "href": "posts/2017-12-20-kerala-workshop.html#install-the-rcpp-package",
    "title": "R Workshop in Kerala",
    "section": "7.3 Install the Rcpp package",
    "text": "7.3 Install the Rcpp package\nSee https://support.rstudio.com/hc/en-us/articles/200486498-Package-Development-Prerequisites for instructions to install the prerequisites (compilers, important libraries, etc). Once the prerequisites are installed, simply install Rcpp as you would any other package.\nTo test your Rcpp environment, try the following in RStudio. 1. Select File =&gt; New File =&gt; C++ File from the main menu. This will create a simple Rcpp program.\n2. Save the program, say, as example.cpp.\n3. Click the Source button, which is located in the upper-right corner of the panel with the example.cpp source code. This should produce the following output.\n&gt; timesTwo(42)\n[1] 84"
  },
  {
    "objectID": "posts/2017-12-20-kerala-workshop.html#install-mpi",
    "href": "posts/2017-12-20-kerala-workshop.html#install-mpi",
    "title": "R Workshop in Kerala",
    "section": "7.4 Install MPI",
    "text": "7.4 Install MPI\nThere are several MPI implementations available, such as OpenMPI and MVAPICH2. The instructions will vary based on your choice of implementation, as well as your hardware and operating system. You are on your own here; good luck! If your installation is successful, you should be able to run the following minimal example.\nFirst, create a new file called hello_mpi.c with the following contents.\n#include &lt;stdio.h&gt;\n#include &lt;mpi.h&gt;\n\nint main (int argc, char *argv[])\n{\n    int id, np;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int processor_name_len;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_size(MPI_COMM_WORLD, &np);\n    MPI_Comm_rank(MPI_COMM_WORLD, &id);\n    MPI_Get_processor_name(processor_name, &processor_name_len);\n\n    printf(\"Hello world from process %03d out of %03d, processor name %s\\n\",\n        id, np, processor_name);\n\n    MPI_Finalize();\n    return 0;\n}\nNow compile and run the program with MPI.\n$ mpicc hello-mpi.c -o hello-mpi\n$ mpirun -np 4 hello-mpi\nHello world from process 000 out of 004, processor name localhost\nHello world from process 001 out of 004, processor name localhost\nHello world from process 002 out of 004, processor name localhost\nHello world from process 003 out of 004, processor name localhost\nDepending on your platform and choice of MPI implementation, your commands may be different. The string localhost in the output will be replaced with your machine’s hostname."
  },
  {
    "objectID": "posts/2017-12-20-kerala-workshop.html#install-r-packages",
    "href": "posts/2017-12-20-kerala-workshop.html#install-r-packages",
    "title": "R Workshop in Kerala",
    "section": "7.5 Install R Packages",
    "text": "7.5 Install R Packages\nDownload the script install.R and run it in R to obtain packages which will be used in the workshop. Suppose we have placed it into the directory /path/to/workshop. Run the following command in R.\n&gt; source(\"/path/to/workshop/install.R\")\nTo make sure your package versions are up to date, open Rstudio and click Tools =&gt; Check for Package Updates on the main menu."
  }
]